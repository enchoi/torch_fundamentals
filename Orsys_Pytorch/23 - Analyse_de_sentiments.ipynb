{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"C5bUWT9PbE2y"},"source":["# Analyse des sentiments"]},{"cell_type":"markdown","metadata":{"id":"Bohe_56hbQ9q"},"source":["Dans ce projet, nous allons utiliser un ensemble de [données Kaggle](https://www.kaggle.com/kazanova/sentiment140) pour l'analyse des sentiments."]},{"cell_type":"markdown","metadata":{"id":"h_tkEBNpbcnK"},"source":["# Importation des données"]},{"cell_type":"markdown","metadata":{"id":"Vyp3YYYmbn8k"},"source":["Ajoutez un raccourci de ce dossier à votre google drive :\n","\n","https://drive.google.com/drive/folders/1uj-BnUzSHJOHuojQ9q8q53DN0EGiPpcl?usp=sharing"]},{"cell_type":"code","metadata":{"id":"xFjOtn0obEl1"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QQZedNAMby-0"},"source":["# Importation des packages"]},{"cell_type":"code","metadata":{"id":"q-8Q64rmaR0L"},"source":["from time import time\n","import numpy as np\n","import pandas as pd\n","# Import Regex to clean up tweets\n","import re\n","\n","import nltk, string\n","from nltk.corpus import stopwords\n","from nltk.tokenize import TweetTokenizer\n","\n","# Get Reviews\n","import requests\n","import json\n","\n","# Get Tweets\n","import httplib2\n","import requests\n","import urllib3\n","from drive.MyDrive.RNN_sentiment_dataset.random_tweets import *\n","\n","# TF IDF Imports\n","\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from scipy.sparse import csc_matrix\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","from joblib import dump, load\n","import torch as torch\n","from torch.utils.data import Dataset\n","import random\n","from torchsummary import summary"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vMN7fVescdmV"},"source":["# Import du dataframe"]},{"cell_type":"code","metadata":{"id":"S6BecGufb26s"},"source":["df = pd.read_csv('/content/drive/MyDrive/RNN_sentiment_dataset/tweets.csv',encoding='latin',usecols=[0, 5], # to take only 2 useful columns\n","                 names=[\"label\",\"tweet\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VPEG3XKecn7G"},"source":["Dans ces données, nous avons y = 0 pour un sentiment négatif et y = 4 pour un sentiment positif.\n","\n","Nous ne garderons que le sentiment positif et négatif et transformerons y pour obtenir 0 pour un sentiment négatif et 1 pour un sentiment positif."]},{"cell_type":"code","metadata":{"id":"VeEP8DU6dNPu"},"source":["df['label'].replace([4, 0],[1, 0], inplace=True) # Replace 0 and 4 by 0 and 1 to clarify"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.shape"],"metadata":{"id":"DT5-hzBkhQRR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2skzLFQPdW48"},"source":["# Prétraitements"]},{"cell_type":"code","metadata":{"id":"Lcy7VAOtdRfv"},"source":["  print(df.head(25))\n","  len(df)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pakc8WAhdUCG"},"source":["Nous pouvons voir que les tweets contiennent des mentions, des urls, etc. qui ne sont pas utiles pour le modèle de langage.\n","\n","Nous devons donc nettoyer leur contenu."]},{"cell_type":"code","source":["small_df = df.iloc[:10000, :]"],"metadata":{"id":"-_PdsdkvhO_L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D98xTSRbdxU5"},"source":["## Nettoyer les tweets"]},{"cell_type":"markdown","metadata":{"id":"WBHJagcUd8JQ"},"source":["Je crée une fonction pour nettoyer les tweets. Les contractions sont séparées, les caractères spéciaux sont supprimés, ainsi que les URL, les mentions, les mots trop courts et les mots vides."]},{"cell_type":"code","metadata":{"id":"DN0WT1PUd4cm"},"source":["def clean(tweet):\n","\n","    # Contractions\n","    tweet = re.sub(r\"he's\", \"he is\", tweet)\n","    tweet = re.sub(r\"there's\", \"there is\", tweet)\n","    tweet = re.sub(r\"We're\", \"We are\", tweet)\n","    tweet = re.sub(r\"That's\", \"That is\", tweet)\n","    tweet = re.sub(r\"won't\", \"will not\", tweet)\n","    tweet = re.sub(r\"they're\", \"they are\", tweet)\n","    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n","    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n","    tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\n","    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n","    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n","    tweet = re.sub(r\"What's\", \"What is\", tweet)\n","    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n","    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n","    tweet = re.sub(r\"There's\", \"There is\", tweet)\n","    tweet = re.sub(r\"He's\", \"He is\", tweet)\n","    tweet = re.sub(r\"It's\", \"It is\", tweet)\n","    tweet = re.sub(r\"You're\", \"You are\", tweet)\n","    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n","    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n","    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n","    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n","    tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\n","    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n","    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n","    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n","    tweet = re.sub(r\"you've\", \"you have\", tweet)\n","    tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\n","    tweet = re.sub(r\"we're\", \"we are\", tweet)\n","    tweet = re.sub(r\"what's\", \"what is\", tweet)\n","    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n","    tweet = re.sub(r\"we've\", \"we have\", tweet)\n","    tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\n","    tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\n","    tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\n","    tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\n","    tweet = re.sub(r\"who's\", \"who is\", tweet)\n","    tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\n","    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n","    tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\n","    tweet = re.sub(r\"would've\", \"would have\", tweet)\n","    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n","    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n","    tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\n","    tweet = re.sub(r\"We've\", \"We have\", tweet)\n","    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n","    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n","    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n","    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n","    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n","    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n","    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n","    tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\n","    tweet = re.sub(r\"they've\", \"they have\", tweet)\n","    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n","    tweet = re.sub(r\"should've\", \"should have\", tweet)\n","    tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\n","    tweet = re.sub(r\"where's\", \"where is\", tweet)\n","    tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\n","    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n","    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n","    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n","    tweet = re.sub(r\"They're\", \"They are\", tweet)\n","    tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\n","    tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\n","    tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\n","    tweet = re.sub(r\"let's\", \"let us\", tweet)\n","    tweet = re.sub(r\"it's\", \"it is\", tweet)\n","    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n","    tweet = re.sub(r\"don't\", \"do not\", tweet)\n","    tweet = re.sub(r\"you're\", \"you are\", tweet)\n","    tweet = re.sub(r\"i've\", \"I have\", tweet)\n","    tweet = re.sub(r\"that's\", \"that is\", tweet)\n","    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n","    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n","    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n","    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n","    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n","    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n","    tweet = re.sub(r\"I've\", \"I have\", tweet)\n","    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n","    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n","    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n","    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n","    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n","    tweet = re.sub(r\"It's\", \"It is\", tweet)\n","    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n","    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n","    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n","    tweet = re.sub(r\"youve\", \"you have\", tweet)\n","    tweet = re.sub(r\"donå«t\", \"do not\", tweet)\n","\n","    tweet = re.sub(r\"some1\", \"someone\", tweet)\n","    tweet = re.sub(r\"yrs\", \"years\", tweet)\n","    tweet = re.sub(r\"hrs\", \"hours\", tweet)\n","    tweet = re.sub(r\"2morow|2moro\", \"tomorrow\", tweet)\n","    tweet = re.sub(r\"2day\", \"today\", tweet)\n","    tweet = re.sub(r\"4got|4gotten\", \"forget\", tweet)\n","    tweet = re.sub(r\"b-day|bday\", \"b-day\", tweet)\n","    tweet = re.sub(r\"mother's\", \"mother\", tweet)\n","    tweet = re.sub(r\"mom's\", \"mom\", tweet)\n","    tweet = re.sub(r\"dad's\", \"dad\", tweet)\n","    tweet = re.sub(r\"hahah|hahaha|hahahaha\", \"haha\", tweet)\n","    tweet = re.sub(r\"lmao|lolz|rofl\", \"lol\", tweet)\n","    tweet = re.sub(r\"thanx|thnx\", \"thanks\", tweet)\n","    tweet = re.sub(r\"goood\", \"good\", tweet)\n","    tweet = re.sub(r\"some1\", \"someone\", tweet)\n","    tweet = re.sub(r\"some1\", \"someone\", tweet)\n","    # Character entity references\n","    tweet = re.sub(r\"&gt;\", \">\", tweet)\n","    tweet = re.sub(r\"&lt;\", \"<\", tweet)\n","    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n","    # Typos, slang and informal abbreviations\n","    tweet = re.sub(r\"w/e\", \"whatever\", tweet)\n","    tweet = re.sub(r\"w/\", \"with\", tweet)\n","    tweet = re.sub(r\"<3\", \"love\", tweet)\n","    # Urls\n","    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n","    # Numbers\n","    tweet = re.sub(r'[0-9]', '', tweet)\n","    # Eliminating the mentions\n","    tweet = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", tweet)\n","    # Remove punctuation and special chars (keep '!')\n","    for p in string.punctuation.replace('!', ''):\n","        tweet = tweet.replace(p, '')\n","\n","    # ... and ..\n","    tweet = tweet.replace('...', ' ... ')\n","    if '...' not in tweet:\n","        tweet = tweet.replace('..', ' ... ')\n","\n","    # join back\n","    #tweet = ' '.join(tweet)\n","\n","\n","    return tweet"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"93unwggneKLH"},"source":["Les abréviations seront remplacées par leur équivalent complet grâce à ce dictionnaire d'abréviations et à la fonction `convert_abbrev_in_text` associée"]},{"cell_type":"code","metadata":{"id":"95WVVnchd6KK"},"source":["variable_name = \"\"\n","abbreviations = {\n","    \"$\" : \" dollar \",\n","    \"€\" : \" euro \",\n","    \"4ao\" : \"for adults only\",\n","    \"a.m\" : \"before midday\",\n","    \"a3\" : \"anytime anywhere anyplace\",\n","    \"aamof\" : \"as a matter of fact\",\n","    \"acct\" : \"account\",\n","    \"adih\" : \"another day in hell\",\n","    \"afaic\" : \"as far as i am concerned\",\n","    \"afaict\" : \"as far as i can tell\",\n","    \"afaik\" : \"as far as i know\",\n","    \"afair\" : \"as far as i remember\",\n","    \"afk\" : \"away from keyboard\",\n","    \"app\" : \"application\",\n","    \"approx\" : \"approximately\",\n","    \"apps\" : \"applications\",\n","    \"asap\" : \"as soon as possible\",\n","    \"asl\" : \"age, sex, location\",\n","    \"atk\" : \"at the keyboard\",\n","    \"ave.\" : \"avenue\",\n","    \"aymm\" : \"are you my mother\",\n","    \"ayor\" : \"at your own risk\",\n","    \"b&b\" : \"bed and breakfast\",\n","    \"b+b\" : \"bed and breakfast\",\n","    \"b.c\" : \"before christ\",\n","    \"b2b\" : \"business to business\",\n","    \"b2c\" : \"business to customer\",\n","    \"b4\" : \"before\",\n","    \"b4n\" : \"bye for now\",\n","    \"b@u\" : \"back at you\",\n","    \"bae\" : \"before anyone else\",\n","    \"bak\" : \"back at keyboard\",\n","    \"bbbg\" : \"bye bye be good\",\n","    \"bbc\" : \"british broadcasting corporation\",\n","    \"bbias\" : \"be back in a second\",\n","    \"bbl\" : \"be back later\",\n","    \"bbs\" : \"be back soon\",\n","    \"be4\" : \"before\",\n","    \"bfn\" : \"bye for now\",\n","    \"blvd\" : \"boulevard\",\n","    \"bout\" : \"about\",\n","    \"brb\" : \"be right back\",\n","    \"bros\" : \"brothers\",\n","    \"brt\" : \"be right there\",\n","    \"bsaaw\" : \"big smile and a wink\",\n","    \"btw\" : \"by the way\",\n","    \"bwl\" : \"bursting with laughter\",\n","    \"c/o\" : \"care of\",\n","    \"cet\" : \"central european time\",\n","    \"cf\" : \"compare\",\n","    \"cia\" : \"central intelligence agency\",\n","    \"csl\" : \"can not stop laughing\",\n","    \"cu\" : \"see you\",\n","    \"cul8r\" : \"see you later\",\n","    \"cv\" : \"curriculum vitae\",\n","    \"cwot\" : \"complete waste of time\",\n","    \"cya\" : \"see you\",\n","    \"cyt\" : \"see you tomorrow\",\n","    \"dae\" : \"does anyone else\",\n","    \"dbmib\" : \"do not bother me i am busy\",\n","    \"diy\" : \"do it yourself\",\n","    \"dm\" : \"direct message\",\n","    \"dwh\" : \"during work hours\",\n","    \"e123\" : \"easy as one two three\",\n","    \"eet\" : \"eastern european time\",\n","    \"eg\" : \"example\",\n","    \"embm\" : \"early morning business meeting\",\n","    \"encl\" : \"enclosed\",\n","    \"encl.\" : \"enclosed\",\n","    \"etc\" : \"and so on\",\n","    \"faq\" : \"frequently asked questions\",\n","    \"fawc\" : \"for anyone who cares\",\n","    \"fb\" : \"facebook\",\n","    \"fc\" : \"fingers crossed\",\n","    \"fig\" : \"figure\",\n","    \"fimh\" : \"forever in my heart\",\n","    \"ft.\" : \"feet\",\n","    \"ft\" : \"featuring\",\n","    \"ftl\" : \"for the loss\",\n","    \"ftw\" : \"for the win\",\n","    \"fwiw\" : \"for what it is worth\",\n","    \"fyi\" : \"for your information\",\n","    \"g9\" : \"genius\",\n","    \"gahoy\" : \"get a hold of yourself\",\n","    \"gal\" : \"get a life\",\n","    \"gcse\" : \"general certificate of secondary education\",\n","    \"gfn\" : \"gone for now\",\n","    \"gg\" : \"good game\",\n","    \"gl\" : \"good luck\",\n","    \"glhf\" : \"good luck have fun\",\n","    \"gmt\" : \"greenwich mean time\",\n","    \"gmta\" : \"great minds think alike\",\n","    \"gn\" : \"good night\",\n","    \"g.o.a.t\" : \"greatest of all time\",\n","    \"goat\" : \"greatest of all time\",\n","    \"goi\" : \"get over it\",\n","    \"gps\" : \"global positioning system\",\n","    \"gr8\" : \"great\",\n","    \"gratz\" : \"congratulations\",\n","    \"gyal\" : \"girl\",\n","    \"h&c\" : \"hot and cold\",\n","    \"hp\" : \"horsepower\",\n","    \"hr\" : \"hour\",\n","    \"hrh\" : \"his royal highness\",\n","    \"ht\" : \"height\",\n","    \"ibrb\" : \"i will be right back\",\n","    \"ic\" : \"i see\",\n","    \"icq\" : \"i seek you\",\n","    \"icymi\" : \"in case you missed it\",\n","    \"idc\" : \"i do not care\",\n","    \"idgadf\" : \"i do not give a damn fuck\",\n","    \"idgaf\" : \"i do not give a fuck\",\n","    \"idk\" : \"i do not know\",\n","    \"ie\" : \"that is\",\n","    \"i.e\" : \"that is\",\n","    \"ifyp\" : \"i feel your pain\",\n","    \"IG\" : \"instagram\",\n","    \"iirc\" : \"if i remember correctly\",\n","    \"ilu\" : \"i love you\",\n","    \"ily\" : \"i love you\",\n","    \"imho\" : \"in my humble opinion\",\n","    \"imo\" : \"in my opinion\",\n","    \"imu\" : \"i miss you\",\n","    \"iow\" : \"in other words\",\n","    \"irl\" : \"in real life\",\n","    \"j4f\" : \"just for fun\",\n","    \"jic\" : \"just in case\",\n","    \"jk\" : \"just kidding\",\n","    \"jsyk\" : \"just so you know\",\n","    \"l8r\" : \"later\",\n","    \"lb\" : \"pound\",\n","    \"lbs\" : \"pounds\",\n","    \"ldr\" : \"long distance relationship\",\n","    \"lmao\" : \"laugh my ass off\",\n","    \"lmfao\" : \"laugh my fucking ass off\",\n","    \"lol\" : \"laughing out loud\",\n","    \"ltd\" : \"limited\",\n","    \"ltns\" : \"long time no see\",\n","    \"m8\" : \"mate\",\n","    \"mf\" : \"motherfucker\",\n","    \"mfs\" : \"motherfuckers\",\n","    \"mfw\" : \"my face when\",\n","    \"mofo\" : \"motherfucker\",\n","    \"mph\" : \"miles per hour\",\n","    \"mr\" : \"mister\",\n","    \"mrw\" : \"my reaction when\",\n","    \"ms\" : \"miss\",\n","    \"mte\" : \"my thoughts exactly\",\n","    \"nagi\" : \"not a good idea\",\n","    \"nbc\" : \"national broadcasting company\",\n","    \"nbd\" : \"not big deal\",\n","    \"nfs\" : \"not for sale\",\n","    \"ngl\" : \"not going to lie\",\n","    \"nhs\" : \"national health service\",\n","    \"nrn\" : \"no reply necessary\",\n","    \"nsfl\" : \"not safe for life\",\n","    \"nsfw\" : \"not safe for work\",\n","    \"nth\" : \"nice to have\",\n","    \"nvr\" : \"never\",\n","    \"nyc\" : \"new york city\",\n","    \"oc\" : \"original content\",\n","    \"og\" : \"original\",\n","    \"ohp\" : \"overhead projector\",\n","    \"oic\" : \"oh i see\",\n","    \"omdb\" : \"over my dead body\",\n","    \"omg\" : \"oh my god\",\n","    \"omw\" : \"on my way\",\n","    \"p.a\" : \"per annum\",\n","    \"p.m\" : \"after midday\",\n","    \"pm\" : \"prime minister\",\n","    \"poc\" : \"people of color\",\n","    \"pov\" : \"point of view\",\n","    \"pp\" : \"pages\",\n","    \"ppl\" : \"people\",\n","    \"prw\" : \"parents are watching\",\n","    \"ps\" : \"postscript\",\n","    \"pt\" : \"point\",\n","    \"ptb\" : \"please text back\",\n","    \"pto\" : \"please turn over\",\n","    \"qpsa\" : \"what happens\",\n","    \"ratchet\" : \"rude\",\n","    \"rbtl\" : \"read between the lines\",\n","    \"rlrt\" : \"real life retweet\",\n","    \"rofl\" : \"rolling on the floor laughing\",\n","    \"roflol\" : \"rolling on the floor laughing out loud\",\n","    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n","    \"rt\" : \"retweet\",\n","    \"ruok\" : \"are you ok\",\n","    \"sfw\" : \"safe for work\",\n","     \"sk8\" : \"skate\",\n","    \"smh\" : \"shake my head\",\n","    \"sq\" : \"square\",\n","    \"srsly\" : \"seriously\",\n","    \"ssdd\" : \"same stuff different day\",\n","    \"tbh\" : \"to be honest\",\n","    \"tbs\" : \"tablespooful\",\n","    \"tbsp\" : \"tablespooful\",\n","    \"tfw\" : \"that feeling when\",\n","    \"thks\" : \"thank you\",\n","    \"tho\" : \"though\",\n","    \"thx\" : \"thank you\",\n","    \"tia\" : \"thanks in advance\",\n","    \"til\" : \"today i learned\",\n","    \"tl;dr\" : \"too long i did not read\",\n","    \"tldr\" : \"too long i did not read\",\n","    \"tmb\" : \"tweet me back\",\n","    \"tntl\" : \"trying not to laugh\",\n","    \"ttyl\" : \"talk to you later\",\n","    \"u\" : \"you\",\n","    \"u2\" : \"you too\",\n","    \"u4e\" : \"yours for ever\",\n","    \"utc\" : \"coordinated universal time\",\n","    \"w/\" : \"with\",\n","    \"w/o\" : \"without\",\n","    \"w8\" : \"wait\",\n","    \"wassup\" : \"what is up\",\n","    \"wb\" : \"welcome back\",\n","    \"wtf\" : \"what the fuck\",\n","    \"wtg\" : \"way to go\",\n","    \"wtpa\" : \"where the party at\",\n","    \"wuf\" : \"where are you from\",\n","    \"wuzup\" : \"what is up\",\n","    \"wywh\" : \"wish you were here\",\n","    \"yd\" : \"yard\",\n","    \"ygtr\" : \"you got that right\",\n","    \"ynk\" : \"you never know\",\n","    \"zzz\" : \"sleeping bored and tired\"\n","}\n","\n","def convert_abbrev_in_text(tweet):\n","    t=[]\n","    words=tweet.split()\n","    t = [abbreviations[w.lower()] if w.lower() in abbreviations.keys() else w for w in words]\n","    return ' '.join(t)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8Wy3Wq-aeONJ"},"source":["La fonction suivante exécute les deux fonctions définies ci-dessus sur un tweet donné :"]},{"cell_type":"code","metadata":{"id":"otc2qVcbeL6B"},"source":["def prepare_string(tweet):\n","  tweet = clean(tweet)\n","  tweet = convert_abbrev_in_text(tweet)\n","  return tweet"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"al9EVADbeRsY"},"source":["Cette étape peut prendre quelques minutes, elle applique la fonction de nettoyage à tous les tweets du corpus de texte et supprime les lignes qui sont vides après le nettoyage."]},{"cell_type":"code","metadata":{"id":"4Iq7Gr0ZeQL7"},"source":["%%time\n","# Apply prepare_string to all rows in 'tweets' column\n","small_df['tweet'] = small_df['tweet'].apply(lambda s : prepare_string(s))\n","\n","# Drop empty values from dataframe\n","small_df['tweet'].replace('', np.nan, inplace=True)\n","small_df.dropna(subset=['tweet'], inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["small_df['tweet']"],"metadata":{"id":"AHnyaDVLqELz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Normalisation"],"metadata":{"id":"BMOsRQZ6m1Ww"}},{"cell_type":"markdown","source":["Mettre tout les mots en minuscules."],"metadata":{"id":"r-9F5mfJy-ZG"}},{"cell_type":"code","source":["%%time\n","small_df['tweet'] = [w.lower() for w in small_df['tweet']]"],"metadata":{"id":"Ha19qgs9yuwd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Tokenize"],"metadata":{"id":"R8tqNo8omDD3"}},{"cell_type":"code","source":["tokenizer = TweetTokenizer(strip_handles=True)"],"metadata":{"id":"ppse4qRjmidU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","small_df['tweet'] = small_df['tweet'].apply(lambda s : tokenizer.tokenize(s))"],"metadata":{"id":"dRExj6NRmWwP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["small_df['tweet']"],"metadata":{"id":"fWvuWh1Qzxu6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Stop words"],"metadata":{"id":"S95jiO4fm6Ag"}},{"cell_type":"code","source":["nltk.download('stopwords')\n","stop_words = nltk.corpus.stopwords.words('english')"],"metadata":{"id":"XejNrIqW1y6n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","for i, s in zip(np.arange(small_df['tweet'].shape[0]), small_df['tweet']):\n","\n","  small_df['tweet'][i] = [w.lower() for w in s if not w in stop_words]"],"metadata":{"id":"NUWHwUUP2MFu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Visualisation du dataframe obtenu"],"metadata":{"id":"VBz4BxcZ22XF"}},{"cell_type":"code","metadata":{"id":"kon3bKO3eXIc"},"source":["small_df.head(25)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Jeu de données complet"],"metadata":{"id":"OzddPzYL6JYq"}},{"cell_type":"markdown","metadata":{"id":"_RQiDkWXeb7d"},"source":["Le DataFrame résultant est converti en CSV et téléchargé afin d'éviter la réexécution du code précédent qui consomme beaucoup de ressources et de temps."]},{"cell_type":"code","metadata":{"id":"cyyYaavmecNj"},"source":["df = pd.read_csv('/content/drive/MyDrive/RNN_sentiment_dataset/cleaned_tweets.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DFj93f1Eeu7x"},"source":["Les données sont maintenant prêtes à être soumises aux différentes méthodes de traitement."]},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"-hBwSRwp588F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5b072kCce8Ip"},"source":["# TF-IDF"]},{"cell_type":"code","metadata":{"id":"QUCSoW0Kfg9T"},"source":["# Using all the data exceeds the RAM capacity of the Notebook\n","corpus_size = int(10000)\n","\n","# Tweets are chosen at the beginning and end of the dataset to have equal parts positive and negative sentiment\n","tweets = [*df['tweet'].values[:int(corpus_size/2)], *df['tweet'].values[-int(corpus_size/2):]]\n","# As well as for the associated targets\n","y = [*df['label'].values[:int(corpus_size/2)], *df['label'].values[-int(corpus_size/2):]]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PPpVRwFzfHj7"},"source":["Utilisez la fonction TF-IDF de Sklearn.\n","\n","Aidez-vous de la [doc](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)."]},{"cell_type":"code","metadata":{"id":"DyPyP9t7fTv-"},"source":["tfIdfVectorizer = None\n","X = None"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OM_CSNzefvRl"},"source":["Divisez votre ensemble de données de formation et de test à l'aide de la fonction sklearn.\n","\n","Aidez-vous de la [doc](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)."]},{"cell_type":"code","metadata":{"id":"BFKso6IbexJP"},"source":["X_train, X_test, y_train, y_test = None"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Création du générateur"],"metadata":{"id":"5ctJ05TryuRH"}},{"cell_type":"markdown","source":["Création d'un dataset custom adapté à notre problématique."],"metadata":{"id":"AWNynonqyzzA"}},{"cell_type":"code","source":["class CustomDataset(Dataset):\n","    def __init__(self, x_train, y_train):\n","        self.input = x_train\n","        self.output = y_train\n","\n","    def __len__(self):\n","        return len(self.output)\n","\n","    def __getitem__(self, idx):\n","        batch_input = self.input[idx]\n","        batch_output = self.output[idx]\n","\n","        return batch_input, batch_output"],"metadata":{"id":"hy3r4YwWuNud"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_training = CustomDataset(torch.from_numpy(np.float32(X_train[:1000])),\n","                                 torch.from_numpy(np.array(np.expand_dims(np.float32(y_train[:1000]), axis=-1))))"],"metadata":{"id":"xn0O6GJHugEZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Utilisez la fonction `DataLoader` avec une taille de batch de 2 pour créer le générateur."],"metadata":{"id":"BT255-D1y5kR"}},{"cell_type":"code","source":["dataloader_train = None"],"metadata":{"id":"WE0T5OKzuguT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Vérification du générateur."],"metadata":{"id":"pJb4kmnMzAzT"}},{"cell_type":"code","source":["for x, y in dataloader_train:\n","  print(x.shape)\n","  print(y.shape)\n","  break"],"metadata":{"id":"HhoiWbSeu-wM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_testing = CustomDataset(torch.from_numpy(np.float32(X_test)),\n","                                 torch.from_numpy(np.array(np.expand_dims(np.float32(y_test), axis=-1))))"],"metadata":{"id":"BCZSJWgyvBc1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Utilisation de la fonction `DataLoader` pour créer le générateur de test, avec une taille de batch de 2."],"metadata":{"id":"PT4pNYQRzDNM"}},{"cell_type":"code","source":["dataloader_test = None"],"metadata":{"id":"9m_9eotpvOZf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Vérification du générateur."],"metadata":{"id":"mSvDbYIazI0j"}},{"cell_type":"code","source":["for x, y in dataloader_test:\n","  print(x.shape)\n","  print(y.shape)\n","  break"],"metadata":{"id":"VgXIPrqPvRSF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Entraînement du modèle"],"metadata":{"id":"0Kv89EzPzLD1"}},{"cell_type":"markdown","source":["Fontion d'entraînement"],"metadata":{"id":"d1qMRdWozMkZ"}},{"cell_type":"code","source":["def number_of_good_prediction(prediction:float, target:int):\n","  one_hot_prediction = np.where(prediction>0.5, 1, 0)\n","  return np.sum(one_hot_prediction == target)"],"metadata":{"id":"idBsG0wwtJ_p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def step(model:torch.nn.Sequential,\n","         opt:torch.optim,\n","         criterion:torch.nn.modules.loss,\n","         x_train:torch.Tensor,\n","         y_train:torch.Tensor,\n","         metric_function)->tuple:\n","  \"\"\"\n","  Executes a single training step for a PyTorch model.\n","  This function performs a forward pass to compute the model's predictions, calculates\n","  the loss between predictions and actual target values, computes gradients for each\n","  model parameter, and updates the parameters using the optimizer.\n","\n","  Args:\n","      model (torch.nn.Sequential): The PyTorch model to train.\n","      optimizer (torch.optim.Optimizer): Optimizer used to update the model's parameters.\n","      criterion (torch.nn.modules.loss._Loss): Loss function used to compute the error.\n","      x_train (torch.Tensor): Input training data (features).\n","      y_train (torch.Tensor): Ground truth labels or target values for the training data.\n","  Returns:\n","      tuple: The updated model and the computed loss for the current step.\n","  \"\"\"\n","\n","  # Réinitialisez les gradients d'optimizer à zéro avec la méthode 'zero_grad'\n","  opt.zero_grad()\n","\n","  # Calculez les prédiction sur le jeu d'entraînement avec la méthode 'froward'\n","  prediction = model.forward(x_train)\n","\n","  # Calculez l'erreur de prédiction avec 'criterion'\n","  loss = criterion(prediction, y_train)\n","\n","  performance = metric_function(prediction.detach().numpy(), y_train.detach().numpy())\n","\n","  # Calculez les gradients avec la méthode 'backward'\n","  loss.backward()\n","\n","  # Mettre à jour les paramètres du modèle avec la méthode 'step'\n","  opt.step()\n","\n","  return model, loss, performance"],"metadata":{"id":"tP2jgi2gtZdd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def fit(model, optimizer, criterion, epoch, trainloader, testloader, metric_function):\n","    epoch = epoch\n","    history_train_loss = []\n","    history_test_loss = []\n","    history_train_metrics = []\n","    history_test_metrics = []\n","\n","    for e in range(epoch) :\n","\n","      train_loss_batch = 0\n","      test_loss_batch = 0\n","      train_metric_batch = 0\n","      test_metric_batch = 0\n","\n","      for images, labels in trainloader:\n","\n","        # mise à jour des poids avec la fonction 'step'\n","        model, train_loss, train_performance = step(model, optimizer, criterion, images, labels, metric_function)\n","\n","        train_loss_batch += train_loss.detach().numpy()\n","\n","        train_metric_batch += train_performance\n","\n","      for images, labels in testloader:\n","\n","        prediction = model.forward(images)\n","\n","        test_loss = criterion(prediction, labels)\n","\n","        test_metric_batch += metric_function(prediction.detach().numpy(), labels.detach().numpy())\n","\n","        test_loss_batch += test_loss.detach().numpy()\n","\n","      train_loss_batch /= len(trainloader.sampler)\n","      test_loss_batch /= len(testloader.sampler)\n","\n","      train_metric_batch /= len(trainloader.sampler)\n","      test_metric_batch /= len(testloader.sampler)\n","\n","      # Sauvegarde des coûts d'entraînement avec append\n","      history_train_loss = np.append(history_train_loss, train_loss_batch)\n","      history_test_loss = np.append(history_test_loss, test_loss_batch)\n","\n","      # Sauvegarde des coûts d'entraînement avec append\n","      history_train_metrics = np.append(history_train_metrics, train_metric_batch)\n","      history_test_metrics = np.append(history_test_metrics, test_metric_batch)\n","\n","      print(f'epoch : {e}/{epoch}')\n","      print('train_loss : '+str(np.squeeze(train_loss_batch))+ ' test_loss : '+str(np.squeeze(test_loss_batch)))\n","      print('train_metric : '+str(np.squeeze(train_metric_batch))+ ' test_metric : '+str(np.squeeze(test_metric_batch)))\n","      print('-------------------------------------------------------------------------------------------------')\n","\n","    return model, history_train_loss, history_test_loss, history_train_metrics, history_test_metrics\n"],"metadata":{"id":"CqDoQiXQta7S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Initialisez le modèle avec la fonction `Sequential` pour obtenir l'architecture suivante :\n","- une couche linéaire avec 64 neurones,\n","- une couche de relu,\n","- une couche linéaire avec 32 neurones,\n","- une couche de relu,\n","- une couche linéaire avec 1 neurone,\n","- une couche Sigmoid."],"metadata":{"id":"FfDnF2xwzO6q"}},{"cell_type":"code","source":["model = None"],"metadata":{"id":"rKCdsBqWtdkf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Utilisez la fontion `summary` pour visualisez le modèle."],"metadata":{"id":"01qmQ-x3z3yP"}},{"cell_type":"code","source":["None"],"metadata":{"id":"wneK-CTZwZjD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Utilisez la fonction `BCELoss` comme fonction de coût.\n","\n","Utilisez la fonction `Adam` comme optimizer avec un learning rate de 0.001."],"metadata":{"id":"sc2QRAKAz5Xg"}},{"cell_type":"code","source":["criterion = None\n","optimizer = None"],"metadata":{"id":"6fhAX_ytw6QA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epoch=5"],"metadata":{"id":"wiCEp_Niw6mH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Utilisez la fonction `fit` pour entraîner le modèle."],"metadata":{"id":"Ux6mlm7z0CwV"}},{"cell_type":"code","source":["model, history_train_loss_deep, history_test_loss_deep, history_train_metrics_deep, history_test_metrics_deep = None"],"metadata":{"id":"tpgzQUAzw23U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8m6k9TJzg6eL"},"source":["Calculer la prédiction sur l'ensemble de test avec la fonction `forward`."]},{"cell_type":"code","metadata":{"id":"J5BWYCO_g9Zq"},"source":["predictions = None"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictions_hot = np.where(predictions>0.5, 1, 0)"],"metadata":{"id":"lmD8k9oJ6REB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JVyMm31-gUp8"},"source":["Calculer quelques mesures de performance :\n","- matrice de confusion ([doc](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)) ;\n","- rapport de classification ([doc](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)) ;\n","- accuracy ([doc](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html))."]},{"cell_type":"code","metadata":{"id":"v5hO6AHdgTRV"},"source":["print(None)\n","\n","print(None)\n","\n","print(None)"],"execution_count":null,"outputs":[]}]}