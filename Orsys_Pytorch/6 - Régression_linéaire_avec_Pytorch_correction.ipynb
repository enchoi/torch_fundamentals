{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"vmh5qAX9HONe"},"source":["# Régression linéaire avec Pytorch\n","\n","Dans ce notebook, nous allons apprendre à utiliser Pytorch pour entraîner un modèle de régression linéaire sur le jeu de données Boston de Scikit-learn.\n","\n","L'objectif est de prédire le prix des maisons à partir de leurs caractéristiques."]},{"cell_type":"markdown","metadata":{"id":"PiOzO53zH8rP"},"source":["## Importation des packages"]},{"cell_type":"code","source":["!pip install scikit-learn==1.1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ICcFSrY0bU7X","executionInfo":{"status":"ok","timestamp":1730127588388,"user_tz":-60,"elapsed":6377,"user":{"displayName":"Gautherot Morgan","userId":"07974205866322024288"}},"outputId":"39cd25b0-40ca-4abc-f2b6-9b4beb79c39e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scikit-learn==1.1 in /usr/local/lib/python3.10/dist-packages (1.1.0)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1) (1.26.4)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1) (1.13.1)\n","Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1) (3.5.0)\n"]}]},{"cell_type":"code","metadata":{"id":"gVBa_RpuGyHU"},"source":["from sklearn.datasets import load_boston\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.preprocessing import StandardScaler\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","from torchsummary import summary"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3mkfIMdaIBhj"},"source":["## Importation des données"]},{"cell_type":"code","source":["boston = load_boston()\n","X = pd.DataFrame(data=boston['data'], columns=boston['feature_names'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"In_VmNQvaZOd","executionInfo":{"status":"ok","timestamp":1730127593905,"user_tz":-60,"elapsed":11,"user":{"displayName":"Gautherot Morgan","userId":"07974205866322024288"}},"outputId":"edf256d8-14b3-49a5-81cb-4241f6d7f1ee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n","\n","    The Boston housing prices dataset has an ethical problem. You can refer to\n","    the documentation of this function for further details.\n","\n","    The scikit-learn maintainers therefore strongly discourage the use of this\n","    dataset unless the purpose of the code is to study and educate about\n","    ethical issues in data science and machine learning.\n","\n","    In this special case, you can fetch the dataset from the original\n","    source::\n","\n","        import pandas as pd\n","        import numpy as np\n","\n","        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n","        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n","        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n","        target = raw_df.values[1::2, 2]\n","\n","    Alternative datasets include the California housing dataset (i.e.\n","    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n","    dataset. You can load the datasets as follows::\n","\n","        from sklearn.datasets import fetch_california_housing\n","        housing = fetch_california_housing()\n","\n","    for the California housing dataset and::\n","\n","        from sklearn.datasets import fetch_openml\n","        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n","\n","    for the Ames housing dataset.\n","  warnings.warn(msg, category=FutureWarning)\n"]}]},{"cell_type":"markdown","source":["Séparation du jeu d'entraînement et du jeu de test"],"metadata":{"id":"fg1amRUMaVVv"}},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split(X, boston['target'], test_size=0.33, random_state=42)"],"metadata":{"id":"MDEie7FwavLn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Normalisation du jeu de données"],"metadata":{"id":"rK0x9yVDaw0i"}},{"cell_type":"code","source":["std_scaler = StandardScaler().fit(X_train, y_train)\n","\n","X_train  =  std_scaler.transform(X_train)\n","X_test = std_scaler.transform(X_test)"],"metadata":{"id":"bCeTIKAKaz-h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Passage de Numpy à Pytorch"],"metadata":{"id":"szY4FS1meVBv"}},{"cell_type":"code","source":["x_train_torch = torch.from_numpy(X_train).to(torch.float32)\n","x_test_torch = torch.from_numpy(X_test).to(torch.float32)\n","\n","y_train_torch = torch.from_numpy(y_train.reshape(-1, 1)).to(torch.float32)\n","y_test_torch = torch.from_numpy(y_test.reshape(-1, 1)).to(torch.float32)"],"metadata":{"id":"r6nNh5SYeYb5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dszlQ3tGJ1ZD"},"source":["## Création de l'architecture"]},{"cell_type":"markdown","metadata":{"id":"YPRWdvgoKMzY"},"source":["La fonction `Linear` permet d'initialiser les poids pour la régression linéaire et d'effectuer une multiplication matricielle entre les poids et les exemples d'entraînement. Utilisez les paramètres `in_features` et `out_features` pour spécifier le nombre d'entrées et de sorties de la couche.\n","\n","Pour plus d'informatons, n'hésitez pas à lire la [documentation](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html).\n","\n","La fonction `Linear` sera déclarée dans un modèle via la fonction `Sequential`.\n","\n","Pour plus d'informatons, n'hésitez pas à lire la [documentation](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html).\n"]},{"cell_type":"code","metadata":{"id":"EBW5KYGZKDeZ"},"source":["def linear_regression(input_shape:int, output_shape:int)->torch.nn.Sequential:\n","  \"\"\"\n","  Creates a simple linear regression model in PyTorch.\n","\n","  This function initializes a linear regression model with a single linear layer.\n","  The linear layer applies an affine transformation to the input data, mapping\n","  from an input space of dimension `input_shape` to an output space of dimension `output_shape`.\n","\n","  Args:\n","      input_shape (int): Number of input features (size of the input layer).\n","      output_shape (int): Number of desired outputs (size of the output layer).\n","\n","  Returns:\n","      torch.nn.Sequential: A PyTorch model containing the specified linear layer.\n","  \"\"\"\n","  model = torch.nn.Sequential(torch.nn.Linear(in_features=input_shape, out_features=output_shape))\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XH8SMd4JLuHJ"},"source":["Initialiser notre modèle en utilisant la fonction précédente `linear_regression`"]},{"cell_type":"code","metadata":{"id":"Raa4Xv5-LwIg"},"source":["rl_model = linear_regression(input_shape=13, output_shape=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Utilisez la fonction `print` pour visualiser l'architecture du modèle."],"metadata":{"id":"KfNqhEUpdfFH"}},{"cell_type":"code","source":["print(rl_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JLmT81jlz7xY","executionInfo":{"status":"ok","timestamp":1730127593905,"user_tz":-60,"elapsed":9,"user":{"displayName":"Gautherot Morgan","userId":"07974205866322024288"}},"outputId":"935c1b5d-3e62-4d19-9da0-2a5bbfdf3226"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sequential(\n","  (0): Linear(in_features=13, out_features=1, bias=True)\n",")\n"]}]},{"cell_type":"markdown","source":["Utilisez la fonction `summary` pour obtenir une visualisation de meilleur qualité.\n","\n","Aidez-vous de la [doc](https://pypi.org/project/torch-summary/)"],"metadata":{"id":"O_D4URYxcIy_"}},{"cell_type":"code","source":["summary(rl_model, (13,))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gcslvZCscfTV","executionInfo":{"status":"ok","timestamp":1730127593906,"user_tz":-60,"elapsed":9,"user":{"displayName":"Gautherot Morgan","userId":"07974205866322024288"}},"outputId":"f85d0395-9325-465e-9d2e-28e64b8008af"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Linear-1                    [-1, 1]              14\n","================================================================\n","Total params: 14\n","Trainable params: 14\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.00\n","Params size (MB): 0.00\n","Estimated Total Size (MB): 0.00\n","----------------------------------------------------------------\n"]}]},{"cell_type":"markdown","metadata":{"id":"ovbgesSnPuJz"},"source":["Prédire notre jeu d'entraînement."]},{"cell_type":"markdown","source":["Utilisez la méthode `forward` du modèle `rl_model` sur les données `x_train_torch` pour appliquer le modèle sur les données."],"metadata":{"id":"sgWiznledgAy"}},{"cell_type":"code","metadata":{"id":"XD7TNaSVPUCO"},"source":["prediction = rl_model.forward(x_train_torch)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8_XrDMfFJ5yg"},"source":["## Définir la fonction de coût"]},{"cell_type":"markdown","metadata":{"id":"TICFolPYPzRU"},"source":["Vous allez maintenant initialiser votre fonction de coût.\n","\n","Pour ce notebook utilisez la mean absolute error qui a pour nom `L1Loss` en Pytorch.\n","\n","N'hésitez pas à lire la [documentation](https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html)."]},{"cell_type":"code","metadata":{"id":"IWDG65LIQWgw"},"source":["criterion = torch.nn.L1Loss()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Appliquez la fonction de coût du nom de `criterion` pour comparer les données prédites par notre modèle `prediction` et les valeurs exactes `y_train_torch`."],"metadata":{"id":"fjYVy_ABgC2L"}},{"cell_type":"code","source":["loss = criterion(prediction, y_train_torch)"],"metadata":{"id":"GJ6FH8-9V-ER"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UroYoaToEPit"},"source":["### Calculer les gradients"]},{"cell_type":"markdown","metadata":{"id":"z8AQ7e9EEPit"},"source":["Vous allez calculer les gradients par rapport à l’erreur pour mettre à jour les poids du modèle.\n","\n","L’objectif est d’ajuster les poids afin de faire converger l’erreur vers 0."]},{"cell_type":"markdown","metadata":{"id":"VuKRpy8MEPit"},"source":["La fonction `backward` de PyTorch permet de calculer automatiquement les gradients qu'importe le graph de calcul que vous avez créé."]},{"cell_type":"markdown","metadata":{"id":"tKMdjHSxEPit"},"source":["Imaginons une matrice de paramètres *w_1* qui va être multiplier par les données *x_1*.\n","\n","On spécifie bien `requires_grad=True` pour les paramètres."]},{"cell_type":"code","metadata":{"id":"2mc0O1rxEPit","outputId":"aad513a3-bc59-4276-e69f-47f00e93ba3c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730127593906,"user_tz":-60,"elapsed":8,"user":{"displayName":"Gautherot Morgan","userId":"07974205866322024288"}}},"source":["w_1 = torch.randn([2, 1], dtype=float, requires_grad=True)\n","x_1 = torch.randn([5, 2], dtype=float)\n","y = torch.randn([5, 1], dtype=float)\n","\n","h = torch.mm(x_1, w_1)\n","print(h.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([5, 1])\n"]}]},{"cell_type":"markdown","metadata":{"id":"7qt8ZvHxEPit"},"source":["Calculons l'erreur de prédiction qui est l'écart entre *y* et *h*."]},{"cell_type":"code","metadata":{"id":"J8mFgfdYEPiu","outputId":"2118d58f-2810-4c43-c8c8-afbe7896e769","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730127593906,"user_tz":-60,"elapsed":7,"user":{"displayName":"Gautherot Morgan","userId":"07974205866322024288"}}},"source":["error = torch.mean(h-y)\n","print(error)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(-0.9974, dtype=torch.float64, grad_fn=<MeanBackward0>)\n"]}]},{"cell_type":"markdown","metadata":{"id":"aqkl37uxEPiu"},"source":["On va maintenant calculer les gradients des poids *w_1*"]},{"cell_type":"code","metadata":{"id":"2ih79psXEPiu","outputId":"2fe50f24-6d20-4688-f69f-b8496dd329ff","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730127593906,"user_tz":-60,"elapsed":6,"user":{"displayName":"Gautherot Morgan","userId":"07974205866322024288"}}},"source":["print(w_1.grad)\n","error.backward()\n","print(w_1.grad)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["None\n","tensor([[0.8467],\n","        [0.1626]], dtype=torch.float64)\n"]}]},{"cell_type":"markdown","metadata":{"id":"xJKdsVgyEPiu"},"source":["On peut voir que les gradients on été initiliasé à *None*, c'est seulement après l'utilisation de la fonction `backward` que les gradients sont calculés."]},{"cell_type":"markdown","source":["Regardons ce que ça donne pour le modèle `rl_model`."],"metadata":{"id":"YrHhQJKihL52"}},{"cell_type":"code","source":["loss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eBzPVUlAgwe7","executionInfo":{"status":"ok","timestamp":1730127594150,"user_tz":-60,"elapsed":249,"user":{"displayName":"Gautherot Morgan","userId":"07974205866322024288"}},"outputId":"d45792d5-274b-4743-f039-283fce917a86"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(23.1111, grad_fn=<MeanBackward0>)"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["On peut voir que la loss caculée précédemment garde des informations sur la fonction de coût et les gradients."],"metadata":{"id":"44lYb1YogvY7"}},{"cell_type":"code","source":["print(rl_model[0].weight.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RQs7Qwreg7O4","executionInfo":{"status":"ok","timestamp":1730127594150,"user_tz":-60,"elapsed":3,"user":{"displayName":"Gautherot Morgan","userId":"07974205866322024288"}},"outputId":"5368455d-acf0-4cad-be2e-379540591c57"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["None\n"]}]},{"cell_type":"markdown","source":["Pour le moment le modèle ne contient aucun gradient."],"metadata":{"id":"ULd0eWODg5Gu"}},{"cell_type":"markdown","source":["Utilisez la méthode `backward` du coût `loss` pour calculer les gradients du modèle `rl_model`."],"metadata":{"id":"2XaFumyLhUVd"}},{"cell_type":"code","metadata":{"id":"_kYm_NdkEPiu"},"source":["loss.backward()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Observez les gradients ainsi obtenus."],"metadata":{"id":"P39dgPM1hlmC"}},{"cell_type":"code","source":["print(rl_model[0].weight.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YiyMdfRYhmn_","executionInfo":{"status":"ok","timestamp":1730127594150,"user_tz":-60,"elapsed":2,"user":{"displayName":"Gautherot Morgan","userId":"07974205866322024288"}},"outputId":"91a1cc40-e113-4fd7-c59f-53700531c1f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-2.1741e-09, -9.9730e-09,  1.6473e-08,  5.4041e-08, -4.6117e-09,\n","         -7.3844e-09,  1.4788e-08,  5.2371e-09,  1.3034e-09, -1.4871e-08,\n","          6.5814e-11,  1.3390e-08,  7.9276e-09]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"ZeeBD4bRh5sH"},"source":["### Optimiser les paramètres du modèle"]},{"cell_type":"markdown","metadata":{"id":"iHJvvdhtEPiu"},"source":["Maintenant que vous avez calculé vos gradients il va falloir mettre à jour les paramètres du modèle.\n","\n","Il existe de nombreux algorithmes d'optimisation, ici on va rester simple en utilisant celui du Stochastic Gradient Descent `SGD`.\n","\n","Vous allez d'abord initialiser l'algorithme d'optimisation avec la fonction `SGD` en paramètre il faudra lui donner les paramètres du modèle `rl_model` avec la méthode `parameters` et un learning rate `lr` de 0.3.\n","\n","N'hésitez pas à regarder la [documentation](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)"]},{"cell_type":"code","metadata":{"id":"xz5Kv3TbEPiv"},"source":["optimizer = torch.optim.SGD(rl_model.parameters(), lr=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Regardez les valeurs des paramètres du modèle."],"metadata":{"id":"jse0h3Zyihou"}},{"cell_type":"code","metadata":{"id":"bUg25te1EPiv","outputId":"749c40ae-03d3-4aef-eabe-03a9a13b8cb8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730127595770,"user_tz":-60,"elapsed":8,"user":{"displayName":"Gautherot Morgan","userId":"07974205866322024288"}}},"source":["print(rl_model[0].weight)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Parameter containing:\n","tensor([[ 0.0691, -0.2716,  0.1170,  0.1510,  0.0740,  0.1648,  0.0294,  0.1783,\n","          0.2528, -0.1496, -0.1378,  0.0181,  0.2523]], requires_grad=True)\n"]}]},{"cell_type":"markdown","source":["Regardez les gradients de chaque paramètre."],"metadata":{"id":"qLBO1dYWikrG"}},{"cell_type":"code","metadata":{"id":"p6gjXYuEEPiv","outputId":"dba1e042-72d7-4094-e6a8-59b1695b2564","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730127595770,"user_tz":-60,"elapsed":8,"user":{"displayName":"Gautherot Morgan","userId":"07974205866322024288"}}},"source":["print(rl_model[0].weight.grad)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-2.1741e-09, -9.9730e-09,  1.6473e-08,  5.4041e-08, -4.6117e-09,\n","         -7.3844e-09,  1.4788e-08,  5.2371e-09,  1.3034e-09, -1.4871e-08,\n","          6.5814e-11,  1.3390e-08,  7.9276e-09]])\n"]}]},{"cell_type":"markdown","source":["Calculez le coût du modèle à ce stade."],"metadata":{"id":"GjaONZmEioZz"}},{"cell_type":"code","source":["loss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dPf0alXLitCQ","executionInfo":{"status":"ok","timestamp":1730127595770,"user_tz":-60,"elapsed":6,"user":{"displayName":"Gautherot Morgan","userId":"07974205866322024288"}},"outputId":"afdccf1f-3da1-487a-b444-dc3da60ade02"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(23.1111, grad_fn=<MeanBackward0>)"]},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","source":["Utilisez la méthode `step` de l'algorithme d'optimisation `optimizer` afin de mettre à jour les paramètres du modèle en utilisant l'algorithme du gradient descent en utilisant les gradients précédemment calculé."],"metadata":{"id":"ewotH5lOit9o"}},{"cell_type":"code","metadata":{"id":"CHBqM49jEPiv"},"source":["optimizer.step()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Regardez les nouvelles valeurs des paramètres du modèle."],"metadata":{"id":"KgAMqITWi7zj"}},{"cell_type":"code","metadata":{"id":"19xyP5A-EPiv","outputId":"6ea527b5-6180-4f42-e505-5575ba8cda1c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730127595770,"user_tz":-60,"elapsed":5,"user":{"displayName":"Gautherot Morgan","userId":"07974205866322024288"}}},"source":["print(rl_model[0].weight)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Parameter containing:\n","tensor([[ 0.0691, -0.2716,  0.1170,  0.1510,  0.0740,  0.1648,  0.0294,  0.1783,\n","          0.2528, -0.1496, -0.1378,  0.0181,  0.2523]], requires_grad=True)\n"]}]},{"cell_type":"markdown","source":["Utilisez la méthode `forward` pour prédire les données avec les nouveaux paramètres du modèle."],"metadata":{"id":"vNe3eMgKjMDO"}},{"cell_type":"code","source":["prediction = rl_model.forward(x_train_torch)"],"metadata":{"id":"YH-b3FaVjEaJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Utilisez `criterion` pour calculer le coût du modèle avec les nouveaux paramètres."],"metadata":{"id":"Us7Rl2KYjSVe"}},{"cell_type":"code","source":["criterion(prediction, y_train_torch)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rC6Pq-pcjXse","executionInfo":{"status":"ok","timestamp":1730127595770,"user_tz":-60,"elapsed":4,"user":{"displayName":"Gautherot Morgan","userId":"07974205866322024288"}},"outputId":"5903299d-aab8-4431-e782-088adfb0bf94"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(22.1111, grad_fn=<MeanBackward0>)"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","source":["On observe qu'avec les nouveaux paramètres, le coût est inférieur au précédent.\n","\n","Le modèle converge donc vers un ensemble de paramètres qui minimise l'erreur de prédiction."],"metadata":{"id":"LvS2QD9njX6m"}},{"cell_type":"markdown","metadata":{"id":"YA6hWkeEJ_ds"},"source":["## Définir la fonction d'entraînement"]},{"cell_type":"markdown","metadata":{"id":"hT8eCpUaSLxz"},"source":["Vous allez maintenant initialiser la fonction d'entraînement du modèle en utilisant le modèle, la fonction de coût et l'algorithme d'optimisation précédemment initialisé."]},{"cell_type":"code","metadata":{"id":"c5iq3weNI2-l"},"source":["def step(model:torch.nn.Sequential,\n","         opt:torch.optim,\n","         criterion:torch.nn.modules.loss,\n","         x_train:torch.Tensor,\n","         y_train:torch.Tensor)->tuple:\n","  \"\"\"\n","  Executes a single training step for a PyTorch model.\n","  This function performs a forward pass to compute the model's predictions, calculates\n","  the loss between predictions and actual target values, computes gradients for each\n","  model parameter, and updates the parameters using the optimizer.\n","\n","  Args:\n","      model (torch.nn.Sequential): The PyTorch model to train.\n","      optimizer (torch.optim.Optimizer): Optimizer used to update the model's parameters.\n","      criterion (torch.nn.modules.loss._Loss): Loss function used to compute the error.\n","      x_train (torch.Tensor): Input training data (features).\n","      y_train (torch.Tensor): Ground truth labels or target values for the training data.\n","  Returns:\n","      tuple: The updated model and the computed loss for the current step.\n","  \"\"\"\n","\n","  # Réinitialisez les gradients d'optimizer à zéro avec la méthode 'zero_grad'\n","  optimizer.zero_grad()\n","\n","  # Calculez les prédiction sur le jeu d'entraînement avec la méthode 'froward'\n","  prediction = model.forward(x_train)\n","\n","  # Calculez l'erreur de prédiction avec 'criterion'\n","  loss = criterion(prediction, y_train)\n","\n","  # Calculez les gradients avec la méthode 'backward'\n","  loss.backward()\n","\n","  # Mettre à jour les paramètres du modèle avec la méthode 'step'\n","  optimizer.step()\n","\n","  return model, loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eP17TJdcTbAT"},"source":["## Entraîner le modèle"]},{"cell_type":"markdown","metadata":{"id":"CDLHMZXrTdGO"},"source":["Il est maintenant temps d'entraîner le modèle\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b9Fl1-rsTKw9","outputId":"6be7dfc1-75af-44c9-935e-abedc9c4020a","executionInfo":{"status":"ok","timestamp":1730127599895,"user_tz":-60,"elapsed":4129,"user":{"displayName":"Gautherot Morgan","userId":"07974205866322024288"}}},"source":["epoch = 1000\n","history_train = []\n","history_test = []\n","\n","for e in range(epoch) :\n","\n","  # mise à jour des poids avec la fonction 'step'\n","  rl_model, train_loss = step(rl_model, optimizer, criterion, x_train_torch, y_train_torch)\n","\n","  # prédiction sur le jeu de test avec la méthode 'foward'\n","  test_pred = rl_model.foward(x_test_torch)\n","\n","  # Calculer l'erreur de prédiction sur le jeu de test avec 'criterion'\n","  test_loss = criterion(test_pred, y_test_torch)\n","\n","  # Sauvegarde des coûts d'entraînement de l'entraînement et du jeu de test avec append\n","  history_train = np.append(history_train, train_loss.detach().numpy())\n","  history_test = np.append(history_test, test_loss.detach().numpy())\n","\n","  print('train_loss : '+str(np.squeeze(train_loss.detach().numpy()))+ ' test_loss : '+str(test_loss.detach().numpy()))\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train_loss : 22.111115 test_loss : 19.823816\n","train_loss : 21.111115 test_loss : 18.823814\n","train_loss : 20.111115 test_loss : 17.823812\n","train_loss : 19.111115 test_loss : 16.844242\n","train_loss : 18.111752 test_loss : 15.872369\n","train_loss : 17.123892 test_loss : 14.909666\n","train_loss : 16.146116 test_loss : 13.959963\n","train_loss : 15.188881 test_loss : 13.022208\n","train_loss : 14.237377 test_loss : 12.086484\n","train_loss : 13.285989 test_loss : 11.158049\n","train_loss : 12.340439 test_loss : 10.238702\n","train_loss : 11.396539 test_loss : 9.319356\n","train_loss : 10.457681 test_loss : 8.403418\n","train_loss : 9.526787 test_loss : 7.5047894\n","train_loss : 8.611082 test_loss : 6.6618695\n","train_loss : 7.7663536 test_loss : 5.954105\n","train_loss : 6.9928327 test_loss : 5.3111453\n","train_loss : 6.2844505 test_loss : 4.7740393\n","train_loss : 5.667228 test_loss : 4.329431\n","train_loss : 5.1213894 test_loss : 3.9980145\n","train_loss : 4.671545 test_loss : 3.7961013\n","train_loss : 4.340914 test_loss : 3.6117272\n","train_loss : 4.083232 test_loss : 3.4621623\n","train_loss : 3.8735428 test_loss : 3.370146\n","train_loss : 3.7209969 test_loss : 3.3014967\n","train_loss : 3.6126456 test_loss : 3.246809\n","train_loss : 3.531209 test_loss : 3.1964118\n","train_loss : 3.4636123 test_loss : 3.1743982\n","train_loss : 3.4029422 test_loss : 3.1404302\n","train_loss : 3.3584633 test_loss : 3.1530294\n","train_loss : 3.3259516 test_loss : 3.1061006\n","train_loss : 3.297941 test_loss : 3.137377\n","train_loss : 3.2776904 test_loss : 3.1087573\n","train_loss : 3.259938 test_loss : 3.1156952\n","train_loss : 3.2483943 test_loss : 3.0997581\n","train_loss : 3.2402318 test_loss : 3.091781\n","train_loss : 3.2336566 test_loss : 3.096666\n","train_loss : 3.2269747 test_loss : 3.1081583\n","train_loss : 3.2208874 test_loss : 3.0962765\n","train_loss : 3.2147408 test_loss : 3.1240866\n","train_loss : 3.2098777 test_loss : 3.0999172\n","train_loss : 3.2031355 test_loss : 3.1129122\n","train_loss : 3.197471 test_loss : 3.1096773\n","train_loss : 3.1920393 test_loss : 3.1239715\n","train_loss : 3.186824 test_loss : 3.1031213\n","train_loss : 3.182369 test_loss : 3.126156\n","train_loss : 3.177999 test_loss : 3.0991871\n","train_loss : 3.1752026 test_loss : 3.1219323\n","train_loss : 3.1699266 test_loss : 3.1141644\n","train_loss : 3.166183 test_loss : 3.1115496\n","train_loss : 3.1626525 test_loss : 3.1158419\n","train_loss : 3.1593828 test_loss : 3.1316056\n","train_loss : 3.1567447 test_loss : 3.1242917\n","train_loss : 3.1534445 test_loss : 3.1374578\n","train_loss : 3.1512446 test_loss : 3.1314857\n","train_loss : 3.1493144 test_loss : 3.1432455\n","train_loss : 3.1486106 test_loss : 3.1233323\n","train_loss : 3.150262 test_loss : 3.161652\n","train_loss : 3.150323 test_loss : 3.1141026\n","train_loss : 3.1534655 test_loss : 3.1800277\n","train_loss : 3.1546128 test_loss : 3.1145046\n","train_loss : 3.1551168 test_loss : 3.1832955\n","train_loss : 3.157398 test_loss : 3.1123164\n","train_loss : 3.1543188 test_loss : 3.1699324\n","train_loss : 3.151976 test_loss : 3.1221178\n","train_loss : 3.1516712 test_loss : 3.182563\n","train_loss : 3.154191 test_loss : 3.1215959\n","train_loss : 3.1524448 test_loss : 3.190992\n","train_loss : 3.157305 test_loss : 3.1178315\n","train_loss : 3.1566184 test_loss : 3.2066596\n","train_loss : 3.1637628 test_loss : 3.1198907\n","train_loss : 3.1558392 test_loss : 3.1878176\n","train_loss : 3.154416 test_loss : 3.1207364\n","train_loss : 3.1504433 test_loss : 3.172508\n","train_loss : 3.149654 test_loss : 3.1290386\n","train_loss : 3.1452672 test_loss : 3.1664271\n","train_loss : 3.1456633 test_loss : 3.141106\n","train_loss : 3.1416771 test_loss : 3.1735454\n","train_loss : 3.1464624 test_loss : 3.128697\n","train_loss : 3.1483703 test_loss : 3.1919549\n","train_loss : 3.1548855 test_loss : 3.1242223\n","train_loss : 3.1529443 test_loss : 3.218082\n","train_loss : 3.167169 test_loss : 3.1253805\n","train_loss : 3.1636884 test_loss : 3.2229867\n","train_loss : 3.1697185 test_loss : 3.1247785\n","train_loss : 3.1628242 test_loss : 3.214654\n","train_loss : 3.163131 test_loss : 3.127383\n","train_loss : 3.1469555 test_loss : 3.1844392\n","train_loss : 3.1508029 test_loss : 3.1369088\n","train_loss : 3.143312 test_loss : 3.1615696\n","train_loss : 3.1424024 test_loss : 3.139628\n","train_loss : 3.1420298 test_loss : 3.1637852\n","train_loss : 3.14201 test_loss : 3.1284494\n","train_loss : 3.1442666 test_loss : 3.1878586\n","train_loss : 3.1534274 test_loss : 3.1206865\n","train_loss : 3.1493373 test_loss : 3.2165892\n","train_loss : 3.168345 test_loss : 3.1212747\n","train_loss : 3.1649566 test_loss : 3.218949\n","train_loss : 3.1683955 test_loss : 3.1216452\n","train_loss : 3.162489 test_loss : 3.2210248\n","train_loss : 3.1688273 test_loss : 3.1230407\n","train_loss : 3.1737514 test_loss : 3.220641\n","train_loss : 3.1703525 test_loss : 3.1171787\n","train_loss : 3.1610446 test_loss : 3.1872272\n","train_loss : 3.1571248 test_loss : 3.1292238\n","train_loss : 3.1417575 test_loss : 3.1520474\n","train_loss : 3.1429431 test_loss : 3.1275547\n","train_loss : 3.1424453 test_loss : 3.155338\n","train_loss : 3.1428392 test_loss : 3.1186168\n","train_loss : 3.145564 test_loss : 3.1709888\n","train_loss : 3.1505473 test_loss : 3.1095164\n","train_loss : 3.1525273 test_loss : 3.1919975\n","train_loss : 3.1616971 test_loss : 3.1105134\n","train_loss : 3.1584811 test_loss : 3.1951802\n","train_loss : 3.1616085 test_loss : 3.1125364\n","train_loss : 3.1474326 test_loss : 3.1591365\n","train_loss : 3.1462584 test_loss : 3.1249657\n","train_loss : 3.1421158 test_loss : 3.1483934\n","train_loss : 3.1418252 test_loss : 3.1137042\n","train_loss : 3.1468687 test_loss : 3.1793199\n","train_loss : 3.1544466 test_loss : 3.1115947\n","train_loss : 3.15208 test_loss : 3.2048643\n","train_loss : 3.1658146 test_loss : 3.1126223\n","train_loss : 3.1640244 test_loss : 3.2091544\n","train_loss : 3.1669059 test_loss : 3.1120594\n","train_loss : 3.1598837 test_loss : 3.2148416\n","train_loss : 3.1719084 test_loss : 3.111352\n","train_loss : 3.1689582 test_loss : 3.2262018\n","train_loss : 3.1807911 test_loss : 3.110985\n","train_loss : 3.1856132 test_loss : 3.1983685\n","train_loss : 3.1663606 test_loss : 3.104154\n","train_loss : 3.1523905 test_loss : 3.148439\n","train_loss : 3.1485415 test_loss : 3.1074944\n","train_loss : 3.143066 test_loss : 3.1389909\n","train_loss : 3.1425345 test_loss : 3.1149998\n","train_loss : 3.1422374 test_loss : 3.1533313\n","train_loss : 3.145884 test_loss : 3.1091776\n","train_loss : 3.1461773 test_loss : 3.161213\n","train_loss : 3.1501987 test_loss : 3.1060858\n","train_loss : 3.1506124 test_loss : 3.1797364\n","train_loss : 3.1581419 test_loss : 3.105078\n","train_loss : 3.1505458 test_loss : 3.159013\n","train_loss : 3.148643 test_loss : 3.1207263\n","train_loss : 3.1413157 test_loss : 3.1504247\n","train_loss : 3.1418777 test_loss : 3.1142395\n","train_loss : 3.14678 test_loss : 3.1703174\n","train_loss : 3.1530359 test_loss : 3.106956\n","train_loss : 3.1570203 test_loss : 3.1898065\n","train_loss : 3.1611047 test_loss : 3.1093223\n","train_loss : 3.1477962 test_loss : 3.1690764\n","train_loss : 3.150244 test_loss : 3.109455\n","train_loss : 3.1486878 test_loss : 3.1661527\n","train_loss : 3.150066 test_loss : 3.1105602\n","train_loss : 3.1499617 test_loss : 3.165501\n","train_loss : 3.1486576 test_loss : 3.1123047\n","train_loss : 3.1460192 test_loss : 3.1681533\n","train_loss : 3.1506116 test_loss : 3.1062527\n","train_loss : 3.1565406 test_loss : 3.1896532\n","train_loss : 3.160378 test_loss : 3.109519\n","train_loss : 3.1482704 test_loss : 3.1571505\n","train_loss : 3.147818 test_loss : 3.119436\n","train_loss : 3.1417463 test_loss : 3.1504176\n","train_loss : 3.141883 test_loss : 3.1137216\n","train_loss : 3.1468768 test_loss : 3.1643991\n","train_loss : 3.148588 test_loss : 3.109902\n","train_loss : 3.1502116 test_loss : 3.172084\n","train_loss : 3.1518037 test_loss : 3.1074336\n","train_loss : 3.1508608 test_loss : 3.1842225\n","train_loss : 3.15711 test_loss : 3.109689\n","train_loss : 3.151052 test_loss : 3.1634536\n","train_loss : 3.1489549 test_loss : 3.1233995\n","train_loss : 3.1407359 test_loss : 3.1344883\n","train_loss : 3.1395109 test_loss : 3.1356022\n","train_loss : 3.1391582 test_loss : 3.149122\n","train_loss : 3.1405137 test_loss : 3.118857\n","train_loss : 3.1454253 test_loss : 3.1604376\n","train_loss : 3.14686 test_loss : 3.1134381\n","train_loss : 3.149134 test_loss : 3.174165\n","train_loss : 3.152292 test_loss : 3.1124885\n","train_loss : 3.1532216 test_loss : 3.197679\n","train_loss : 3.1614017 test_loss : 3.115436\n","train_loss : 3.1526337 test_loss : 3.1747973\n","train_loss : 3.1516747 test_loss : 3.1127985\n","train_loss : 3.1501198 test_loss : 3.167034\n","train_loss : 3.148853 test_loss : 3.11237\n","train_loss : 3.1473186 test_loss : 3.161307\n","train_loss : 3.1472535 test_loss : 3.1122134\n","train_loss : 3.1445124 test_loss : 3.152654\n","train_loss : 3.145623 test_loss : 3.1185603\n","train_loss : 3.143093 test_loss : 3.1534007\n","train_loss : 3.1432068 test_loss : 3.1125324\n","train_loss : 3.1441345 test_loss : 3.1544354\n","train_loss : 3.1448503 test_loss : 3.119129\n","train_loss : 3.1449652 test_loss : 3.1742246\n","train_loss : 3.1526177 test_loss : 3.114997\n","train_loss : 3.148942 test_loss : 3.196159\n","train_loss : 3.1583612 test_loss : 3.1179376\n","train_loss : 3.150158 test_loss : 3.1810431\n","train_loss : 3.1523952 test_loss : 3.1160636\n","train_loss : 3.1473663 test_loss : 3.1764114\n","train_loss : 3.151528 test_loss : 3.1154504\n","train_loss : 3.1525273 test_loss : 3.2025497\n","train_loss : 3.1619208 test_loss : 3.1192656\n","train_loss : 3.1471717 test_loss : 3.1573248\n","train_loss : 3.1462607 test_loss : 3.1245904\n","train_loss : 3.1416748 test_loss : 3.1542544\n","train_loss : 3.1423812 test_loss : 3.1215205\n","train_loss : 3.1430345 test_loss : 3.1600163\n","train_loss : 3.1443288 test_loss : 3.1152418\n","train_loss : 3.151765 test_loss : 3.1882086\n","train_loss : 3.1571138 test_loss : 3.1142168\n","train_loss : 3.1518505 test_loss : 3.1822765\n","train_loss : 3.1526685 test_loss : 3.1158726\n","train_loss : 3.1518352 test_loss : 3.1752532\n","train_loss : 3.1502035 test_loss : 3.113404\n","train_loss : 3.1525126 test_loss : 3.1896713\n","train_loss : 3.1576288 test_loss : 3.114938\n","train_loss : 3.1507535 test_loss : 3.1690536\n","train_loss : 3.1484983 test_loss : 3.1288276\n","train_loss : 3.1407106 test_loss : 3.1501553\n","train_loss : 3.1412423 test_loss : 3.1150494\n","train_loss : 3.14614 test_loss : 3.1610348\n","train_loss : 3.1472635 test_loss : 3.1115997\n","train_loss : 3.1485276 test_loss : 3.1697314\n","train_loss : 3.149637 test_loss : 3.1132395\n","train_loss : 3.1490211 test_loss : 3.1712077\n","train_loss : 3.1498988 test_loss : 3.1090147\n","train_loss : 3.1504903 test_loss : 3.1803067\n","train_loss : 3.1536264 test_loss : 3.109895\n","train_loss : 3.1484952 test_loss : 3.16952\n","train_loss : 3.1500478 test_loss : 3.106682\n","train_loss : 3.1493063 test_loss : 3.180254\n","train_loss : 3.1549666 test_loss : 3.1085665\n","train_loss : 3.1530585 test_loss : 3.1924946\n","train_loss : 3.160475 test_loss : 3.110538\n","train_loss : 3.153078 test_loss : 3.1765454\n","train_loss : 3.1550796 test_loss : 3.1202188\n","train_loss : 3.1428363 test_loss : 3.1481264\n","train_loss : 3.1417582 test_loss : 3.1278734\n","train_loss : 3.1414316 test_loss : 3.1480153\n","train_loss : 3.1415148 test_loss : 3.1211333\n","train_loss : 3.1424453 test_loss : 3.157378\n","train_loss : 3.144223 test_loss : 3.1140075\n","train_loss : 3.1502123 test_loss : 3.171081\n","train_loss : 3.1518915 test_loss : 3.1105015\n","train_loss : 3.1511123 test_loss : 3.1859076\n","train_loss : 3.1567085 test_loss : 3.1127572\n","train_loss : 3.1512551 test_loss : 3.1653442\n","train_loss : 3.148662 test_loss : 3.1260545\n","train_loss : 3.1408148 test_loss : 3.1526785\n","train_loss : 3.1419368 test_loss : 3.1161988\n","train_loss : 3.14691 test_loss : 3.1664038\n","train_loss : 3.1490726 test_loss : 3.1128442\n","train_loss : 3.1497831 test_loss : 3.1811738\n","train_loss : 3.1545472 test_loss : 3.1108818\n","train_loss : 3.1535616 test_loss : 3.1921585\n","train_loss : 3.159878 test_loss : 3.112942\n","train_loss : 3.1503417 test_loss : 3.168455\n","train_loss : 3.14882 test_loss : 3.1130064\n","train_loss : 3.1455948 test_loss : 3.1513214\n","train_loss : 3.1461258 test_loss : 3.1184995\n","train_loss : 3.1418974 test_loss : 3.1440992\n","train_loss : 3.1400683 test_loss : 3.141937\n","train_loss : 3.1394594 test_loss : 3.1219988\n","train_loss : 3.1409872 test_loss : 3.1424692\n","train_loss : 3.1404698 test_loss : 3.1301956\n","train_loss : 3.1393762 test_loss : 3.1387348\n","train_loss : 3.139123 test_loss : 3.1195376\n","train_loss : 3.141114 test_loss : 3.1420941\n","train_loss : 3.1410666 test_loss : 3.1246886\n","train_loss : 3.140484 test_loss : 3.146768\n","train_loss : 3.1415043 test_loss : 3.1130357\n","train_loss : 3.147251 test_loss : 3.1522102\n","train_loss : 3.1447308 test_loss : 3.1084747\n","train_loss : 3.1519098 test_loss : 3.1814704\n","train_loss : 3.155929 test_loss : 3.1097755\n","train_loss : 3.152979 test_loss : 3.1962104\n","train_loss : 3.1617885 test_loss : 3.1125076\n","train_loss : 3.152497 test_loss : 3.1730661\n","train_loss : 3.151777 test_loss : 3.1098254\n","train_loss : 3.1500165 test_loss : 3.1714702\n","train_loss : 3.151816 test_loss : 3.1082683\n","train_loss : 3.15247 test_loss : 3.1879225\n","train_loss : 3.1589463 test_loss : 3.1117046\n","train_loss : 3.1495516 test_loss : 3.1667356\n","train_loss : 3.1501791 test_loss : 3.124349\n","train_loss : 3.141107 test_loss : 3.1566095\n","train_loss : 3.1436536 test_loss : 3.1154797\n","train_loss : 3.1488495 test_loss : 3.1576457\n","train_loss : 3.1455805 test_loss : 3.1211472\n","train_loss : 3.1428356 test_loss : 3.1638227\n","train_loss : 3.1480403 test_loss : 3.1129725\n","train_loss : 3.1481378 test_loss : 3.1788578\n","train_loss : 3.1535215 test_loss : 3.1143372\n","train_loss : 3.1551223 test_loss : 3.1978629\n","train_loss : 3.159847 test_loss : 3.1175988\n","train_loss : 3.1490443 test_loss : 3.1597455\n","train_loss : 3.1460319 test_loss : 3.113502\n","train_loss : 3.1460207 test_loss : 3.170766\n","train_loss : 3.1496754 test_loss : 3.114738\n","train_loss : 3.1491706 test_loss : 3.172697\n","train_loss : 3.1494417 test_loss : 3.1100307\n","train_loss : 3.150956 test_loss : 3.1874526\n","train_loss : 3.1570451 test_loss : 3.112286\n","train_loss : 3.1510499 test_loss : 3.1670902\n","train_loss : 3.148346 test_loss : 3.11268\n","train_loss : 3.1446779 test_loss : 3.1418397\n","train_loss : 3.1424377 test_loss : 3.1193726\n","train_loss : 3.1413968 test_loss : 3.1489346\n","train_loss : 3.1414814 test_loss : 3.1310184\n","train_loss : 3.1405551 test_loss : 3.1530836\n","train_loss : 3.141761 test_loss : 3.1201794\n","train_loss : 3.1429577 test_loss : 3.1585665\n","train_loss : 3.144932 test_loss : 3.1139977\n","train_loss : 3.1516325 test_loss : 3.1866214\n","train_loss : 3.1573904 test_loss : 3.1126978\n","train_loss : 3.151579 test_loss : 3.1878355\n","train_loss : 3.156677 test_loss : 3.1150713\n","train_loss : 3.151608 test_loss : 3.172862\n","train_loss : 3.149863 test_loss : 3.1144288\n","train_loss : 3.1444294 test_loss : 3.1555328\n","train_loss : 3.1463482 test_loss : 3.1215389\n","train_loss : 3.1422706 test_loss : 3.1579938\n","train_loss : 3.144321 test_loss : 3.1133647\n","train_loss : 3.1498454 test_loss : 3.1765852\n","train_loss : 3.1528313 test_loss : 3.1104333\n","train_loss : 3.1544352 test_loss : 3.1893494\n","train_loss : 3.157578 test_loss : 3.1138217\n","train_loss : 3.1508865 test_loss : 3.1685524\n","train_loss : 3.1485538 test_loss : 3.1160827\n","train_loss : 3.1437411 test_loss : 3.1506493\n","train_loss : 3.1431866 test_loss : 3.12375\n","train_loss : 3.1410546 test_loss : 3.143135\n","train_loss : 3.1398187 test_loss : 3.14097\n","train_loss : 3.1390932 test_loss : 3.1237423\n","train_loss : 3.1406715 test_loss : 3.1444085\n","train_loss : 3.1401882 test_loss : 3.1398275\n","train_loss : 3.1394105 test_loss : 3.141179\n","train_loss : 3.1395166 test_loss : 3.1212616\n","train_loss : 3.1414363 test_loss : 3.1436224\n","train_loss : 3.1407857 test_loss : 3.12859\n","train_loss : 3.1398973 test_loss : 3.152797\n","train_loss : 3.14221 test_loss : 3.11491\n","train_loss : 3.1480196 test_loss : 3.152933\n","train_loss : 3.1443279 test_loss : 3.1104217\n","train_loss : 3.15229 test_loss : 3.1972444\n","train_loss : 3.1633391 test_loss : 3.1121044\n","train_loss : 3.1560607 test_loss : 3.1789126\n","train_loss : 3.1545966 test_loss : 3.1101904\n","train_loss : 3.153705 test_loss : 3.1656446\n","train_loss : 3.1489568 test_loss : 3.1225715\n","train_loss : 3.143111 test_loss : 3.165849\n","train_loss : 3.147998 test_loss : 3.1152625\n","train_loss : 3.1440234 test_loss : 3.1548326\n","train_loss : 3.1432126 test_loss : 3.1121736\n","train_loss : 3.146135 test_loss : 3.1664944\n","train_loss : 3.14735 test_loss : 3.114635\n","train_loss : 3.148107 test_loss : 3.1751354\n","train_loss : 3.1512823 test_loss : 3.110095\n","train_loss : 3.1552973 test_loss : 3.194761\n","train_loss : 3.159514 test_loss : 3.114374\n","train_loss : 3.1493695 test_loss : 3.1724193\n","train_loss : 3.149922 test_loss : 3.1113372\n","train_loss : 3.1520326 test_loss : 3.1858149\n","train_loss : 3.1559289 test_loss : 3.113593\n","train_loss : 3.1520755 test_loss : 3.1722445\n","train_loss : 3.1500802 test_loss : 3.1110423\n","train_loss : 3.1496813 test_loss : 3.1667597\n","train_loss : 3.149107 test_loss : 3.1237311\n","train_loss : 3.1427996 test_loss : 3.1564612\n","train_loss : 3.14298 test_loss : 3.118155\n","train_loss : 3.144259 test_loss : 3.1538997\n","train_loss : 3.143693 test_loss : 3.1230834\n","train_loss : 3.1442702 test_loss : 3.1669447\n","train_loss : 3.148604 test_loss : 3.115026\n","train_loss : 3.1451619 test_loss : 3.163385\n","train_loss : 3.1481724 test_loss : 3.1112945\n","train_loss : 3.151664 test_loss : 3.1890123\n","train_loss : 3.1569471 test_loss : 3.1141372\n","train_loss : 3.151504 test_loss : 3.1902266\n","train_loss : 3.1565797 test_loss : 3.1169455\n","train_loss : 3.1460588 test_loss : 3.158188\n","train_loss : 3.1463249 test_loss : 3.12392\n","train_loss : 3.1420884 test_loss : 3.156787\n","train_loss : 3.1428497 test_loss : 3.1170566\n","train_loss : 3.1448448 test_loss : 3.157728\n","train_loss : 3.1442075 test_loss : 3.111888\n","train_loss : 3.154163 test_loss : 3.1914952\n","train_loss : 3.1585622 test_loss : 3.115151\n","train_loss : 3.150502 test_loss : 3.1705403\n","train_loss : 3.1486146 test_loss : 3.1141343\n","train_loss : 3.1475253 test_loss : 3.1758204\n","train_loss : 3.1514702 test_loss : 3.1109283\n","train_loss : 3.1532476 test_loss : 3.1844158\n","train_loss : 3.1549797 test_loss : 3.1131837\n","train_loss : 3.1530714 test_loss : 3.193598\n","train_loss : 3.1580422 test_loss : 3.1170814\n","train_loss : 3.1469493 test_loss : 3.155394\n","train_loss : 3.1469169 test_loss : 3.1186922\n","train_loss : 3.1424263 test_loss : 3.1489897\n","train_loss : 3.1415365 test_loss : 3.1356306\n","train_loss : 3.1393132 test_loss : 3.1574068\n","train_loss : 3.1420665 test_loss : 3.1194596\n","train_loss : 3.1462128 test_loss : 3.1709247\n","train_loss : 3.1503718 test_loss : 3.1153517\n","train_loss : 3.1493363 test_loss : 3.178757\n","train_loss : 3.152975 test_loss : 3.112112\n","train_loss : 3.1542058 test_loss : 3.199748\n","train_loss : 3.163208 test_loss : 3.1122923\n","train_loss : 3.1610198 test_loss : 3.2066402\n","train_loss : 3.1664674 test_loss : 3.1119406\n","train_loss : 3.1683562 test_loss : 3.228026\n","train_loss : 3.1818697 test_loss : 3.1088674\n","train_loss : 3.1687045 test_loss : 3.223233\n","train_loss : 3.1803198 test_loss : 3.1053972\n","train_loss : 3.163881 test_loss : 3.1845074\n","train_loss : 3.1592467 test_loss : 3.1060202\n","train_loss : 3.1460044 test_loss : 3.1389194\n","train_loss : 3.1437302 test_loss : 3.114388\n","train_loss : 3.142545 test_loss : 3.1480513\n","train_loss : 3.1423528 test_loss : 3.1206145\n","train_loss : 3.1427171 test_loss : 3.158015\n","train_loss : 3.146057 test_loss : 3.10915\n","train_loss : 3.1480157 test_loss : 3.153949\n","train_loss : 3.1463025 test_loss : 3.105034\n","train_loss : 3.1471555 test_loss : 3.1669996\n","train_loss : 3.1515818 test_loss : 3.1035128\n","train_loss : 3.1541471 test_loss : 3.1885135\n","train_loss : 3.1603494 test_loss : 3.1072366\n","train_loss : 3.148597 test_loss : 3.1660938\n","train_loss : 3.1507475 test_loss : 3.105891\n","train_loss : 3.148616 test_loss : 3.1642287\n","train_loss : 3.1495178 test_loss : 3.1155572\n","train_loss : 3.1454582 test_loss : 3.1579046\n","train_loss : 3.1473312 test_loss : 3.1093574\n","train_loss : 3.1445067 test_loss : 3.14651\n","train_loss : 3.1432464 test_loss : 3.1174035\n","train_loss : 3.1413178 test_loss : 3.1509337\n","train_loss : 3.143842 test_loss : 3.113017\n","train_loss : 3.1464856 test_loss : 3.1604564\n","train_loss : 3.1467857 test_loss : 3.1091316\n","train_loss : 3.1488495 test_loss : 3.1684544\n","train_loss : 3.150335 test_loss : 3.110963\n","train_loss : 3.1526647 test_loss : 3.193544\n","train_loss : 3.1591763 test_loss : 3.1134088\n","train_loss : 3.1494312 test_loss : 3.1716835\n","train_loss : 3.1501105 test_loss : 3.109534\n","train_loss : 3.1461535 test_loss : 3.1735907\n","train_loss : 3.1529582 test_loss : 3.1087928\n","train_loss : 3.1509185 test_loss : 3.1858234\n","train_loss : 3.1569655 test_loss : 3.1105661\n","train_loss : 3.1510553 test_loss : 3.1780658\n","train_loss : 3.1542745 test_loss : 3.1097722\n","train_loss : 3.147704 test_loss : 3.168885\n","train_loss : 3.1508112 test_loss : 3.1077745\n","train_loss : 3.1504238 test_loss : 3.1613858\n","train_loss : 3.1486204 test_loss : 3.1228788\n","train_loss : 3.14151 test_loss : 3.1486816\n","train_loss : 3.1408825 test_loss : 3.1168406\n","train_loss : 3.1435552 test_loss : 3.154421\n","train_loss : 3.1439068 test_loss : 3.1109338\n","train_loss : 3.1526766 test_loss : 3.1981041\n","train_loss : 3.1625307 test_loss : 3.1124692\n","train_loss : 3.151069 test_loss : 3.16655\n","train_loss : 3.1479032 test_loss : 3.1114528\n","train_loss : 3.1480925 test_loss : 3.171859\n","train_loss : 3.1509764 test_loss : 3.1082463\n","train_loss : 3.1533928 test_loss : 3.1909106\n","train_loss : 3.1603525 test_loss : 3.110187\n","train_loss : 3.1536329 test_loss : 3.1725047\n","train_loss : 3.1511152 test_loss : 3.1088667\n","train_loss : 3.1482878 test_loss : 3.1600404\n","train_loss : 3.148771 test_loss : 3.1196892\n","train_loss : 3.1424696 test_loss : 3.149065\n","train_loss : 3.1418695 test_loss : 3.1137922\n","train_loss : 3.1457317 test_loss : 3.160975\n","train_loss : 3.147564 test_loss : 3.11\n","train_loss : 3.1482341 test_loss : 3.1753566\n","train_loss : 3.1539524 test_loss : 3.1065865\n","train_loss : 3.1547282 test_loss : 3.190097\n","train_loss : 3.1597712 test_loss : 3.1108007\n","train_loss : 3.1487656 test_loss : 3.1684508\n","train_loss : 3.1503687 test_loss : 3.1174865\n","train_loss : 3.1435695 test_loss : 3.1476982\n","train_loss : 3.142302 test_loss : 3.1168044\n","train_loss : 3.142257 test_loss : 3.1538074\n","train_loss : 3.1435568 test_loss : 3.1105695\n","train_loss : 3.1503067 test_loss : 3.1656501\n","train_loss : 3.1486356 test_loss : 3.1106453\n","train_loss : 3.1500602 test_loss : 3.1677659\n","train_loss : 3.1487262 test_loss : 3.1070077\n","train_loss : 3.1517544 test_loss : 3.1755366\n","train_loss : 3.1525638 test_loss : 3.108663\n","train_loss : 3.1517434 test_loss : 3.1758573\n","train_loss : 3.1534126 test_loss : 3.1076992\n","train_loss : 3.1506715 test_loss : 3.168389\n","train_loss : 3.151162 test_loss : 3.1159718\n","train_loss : 3.146057 test_loss : 3.1623302\n","train_loss : 3.1490421 test_loss : 3.1114523\n","train_loss : 3.144842 test_loss : 3.1423848\n","train_loss : 3.1417778 test_loss : 3.128699\n","train_loss : 3.1397886 test_loss : 3.1419704\n","train_loss : 3.1398768 test_loss : 3.1188593\n","train_loss : 3.1421578 test_loss : 3.148502\n","train_loss : 3.1420016 test_loss : 3.1205177\n","train_loss : 3.1456616 test_loss : 3.1740654\n","train_loss : 3.1520271 test_loss : 3.116564\n","train_loss : 3.14936 test_loss : 3.1686122\n","train_loss : 3.1497645 test_loss : 3.1123\n","train_loss : 3.1483812 test_loss : 3.1710227\n","train_loss : 3.150397 test_loss : 3.1149168\n","train_loss : 3.1447048 test_loss : 3.1490943\n","train_loss : 3.143509 test_loss : 3.1190338\n","train_loss : 3.1425078 test_loss : 3.1540918\n","train_loss : 3.1432316 test_loss : 3.1131372\n","train_loss : 3.143333 test_loss : 3.1486163\n","train_loss : 3.1419644 test_loss : 3.1321616\n","train_loss : 3.1397223 test_loss : 3.1477365\n","train_loss : 3.1409233 test_loss : 3.1233425\n","train_loss : 3.1410673 test_loss : 3.146989\n","train_loss : 3.1409059 test_loss : 3.1371043\n","train_loss : 3.1393712 test_loss : 3.1444297\n","train_loss : 3.1396973 test_loss : 3.1239955\n","train_loss : 3.1412387 test_loss : 3.143982\n","train_loss : 3.1408951 test_loss : 3.128064\n","train_loss : 3.1395187 test_loss : 3.1490302\n","train_loss : 3.141443 test_loss : 3.11998\n","train_loss : 3.1412058 test_loss : 3.1387897\n","train_loss : 3.1407986 test_loss : 3.1274545\n","train_loss : 3.1398392 test_loss : 3.141588\n","train_loss : 3.1399539 test_loss : 3.1213415\n","train_loss : 3.1409285 test_loss : 3.1308389\n","train_loss : 3.1399164 test_loss : 3.1303215\n","train_loss : 3.1390312 test_loss : 3.1376426\n","train_loss : 3.1397576 test_loss : 3.118077\n","train_loss : 3.1413298 test_loss : 3.137096\n","train_loss : 3.1408563 test_loss : 3.122145\n","train_loss : 3.1396344 test_loss : 3.138416\n","train_loss : 3.1399639 test_loss : 3.1191182\n","train_loss : 3.1404474 test_loss : 3.1284125\n","train_loss : 3.1402037 test_loss : 3.1271672\n","train_loss : 3.139029 test_loss : 3.134592\n","train_loss : 3.139485 test_loss : 3.1170456\n","train_loss : 3.141793 test_loss : 3.16018\n","train_loss : 3.146639 test_loss : 3.1125295\n","train_loss : 3.14929 test_loss : 3.174243\n","train_loss : 3.1530318 test_loss : 3.1090193\n","train_loss : 3.1544826 test_loss : 3.1958106\n","train_loss : 3.1627798 test_loss : 3.1113794\n","train_loss : 3.1468039 test_loss : 3.1526194\n","train_loss : 3.147176 test_loss : 3.1166823\n","train_loss : 3.1417277 test_loss : 3.150303\n","train_loss : 3.1436338 test_loss : 3.1095471\n","train_loss : 3.1486762 test_loss : 3.168003\n","train_loss : 3.1506135 test_loss : 3.1069489\n","train_loss : 3.157527 test_loss : 3.188786\n","train_loss : 3.1603053 test_loss : 3.109408\n","train_loss : 3.148438 test_loss : 3.1739037\n","train_loss : 3.1540995 test_loss : 3.1073546\n","train_loss : 3.1481228 test_loss : 3.1658292\n","train_loss : 3.1507144 test_loss : 3.109635\n","train_loss : 3.146296 test_loss : 3.1718364\n","train_loss : 3.1519544 test_loss : 3.108878\n","train_loss : 3.1494148 test_loss : 3.164527\n","train_loss : 3.1493564 test_loss : 3.1085896\n","train_loss : 3.144609 test_loss : 3.1353815\n","train_loss : 3.1418018 test_loss : 3.1282892\n","train_loss : 3.1395497 test_loss : 3.1284215\n","train_loss : 3.139482 test_loss : 3.1412964\n","train_loss : 3.1392956 test_loss : 3.121957\n","train_loss : 3.1405218 test_loss : 3.1449597\n","train_loss : 3.1415648 test_loss : 3.1292565\n","train_loss : 3.139252 test_loss : 3.1365795\n","train_loss : 3.139184 test_loss : 3.1371906\n","train_loss : 3.139033 test_loss : 3.1243272\n","train_loss : 3.1395497 test_loss : 3.1249025\n","train_loss : 3.140118 test_loss : 3.1480827\n","train_loss : 3.1422086 test_loss : 3.1113698\n","train_loss : 3.146265 test_loss : 3.1538167\n","train_loss : 3.1456666 test_loss : 3.117546\n","train_loss : 3.147087 test_loss : 3.168108\n","train_loss : 3.150095 test_loss : 3.1123426\n","train_loss : 3.148638 test_loss : 3.1836836\n","train_loss : 3.156223 test_loss : 3.111355\n","train_loss : 3.152316 test_loss : 3.1992676\n","train_loss : 3.162003 test_loss : 3.1160889\n","train_loss : 3.1430585 test_loss : 3.1443598\n","train_loss : 3.1413276 test_loss : 3.1243677\n","train_loss : 3.1412094 test_loss : 3.1501238\n","train_loss : 3.1417637 test_loss : 3.1187704\n","train_loss : 3.1432025 test_loss : 3.1534271\n","train_loss : 3.1438723 test_loss : 3.1090252\n","train_loss : 3.1507902 test_loss : 3.199116\n","train_loss : 3.1648269 test_loss : 3.1096275\n","train_loss : 3.1596498 test_loss : 3.200436\n","train_loss : 3.165375 test_loss : 3.1114953\n","train_loss : 3.1489394 test_loss : 3.1536\n","train_loss : 3.147314 test_loss : 3.1170852\n","train_loss : 3.143055 test_loss : 3.1570578\n","train_loss : 3.1461709 test_loss : 3.1106348\n","train_loss : 3.1455045 test_loss : 3.1550457\n","train_loss : 3.1455078 test_loss : 3.1199415\n","train_loss : 3.1441278 test_loss : 3.164657\n","train_loss : 3.148335 test_loss : 3.111946\n","train_loss : 3.1450922 test_loss : 3.1777604\n","train_loss : 3.1545663 test_loss : 3.1103125\n","train_loss : 3.1542919 test_loss : 3.1963904\n","train_loss : 3.1612217 test_loss : 3.1139328\n","train_loss : 3.1476943 test_loss : 3.1806645\n","train_loss : 3.1550174 test_loss : 3.1118789\n","train_loss : 3.147309 test_loss : 3.1717088\n","train_loss : 3.1514313 test_loss : 3.1131635\n","train_loss : 3.1455958 test_loss : 3.1661131\n","train_loss : 3.1480079 test_loss : 3.112194\n","train_loss : 3.144236 test_loss : 3.1526375\n","train_loss : 3.1458578 test_loss : 3.1165595\n","train_loss : 3.143689 test_loss : 3.1651871\n","train_loss : 3.1486316 test_loss : 3.1119897\n","train_loss : 3.15066 test_loss : 3.2055533\n","train_loss : 3.164414 test_loss : 3.1135392\n","train_loss : 3.154415 test_loss : 3.1955743\n","train_loss : 3.159632 test_loss : 3.1174994\n","train_loss : 3.1443977 test_loss : 3.1583953\n","train_loss : 3.1452672 test_loss : 3.1230044\n","train_loss : 3.141728 test_loss : 3.1594424\n","train_loss : 3.1449213 test_loss : 3.1140094\n","train_loss : 3.1490612 test_loss : 3.1782255\n","train_loss : 3.1538622 test_loss : 3.1109984\n","train_loss : 3.15362 test_loss : 3.1974344\n","train_loss : 3.161405 test_loss : 3.1146262\n","train_loss : 3.1477938 test_loss : 3.158792\n","train_loss : 3.1460745 test_loss : 3.1116965\n","train_loss : 3.1458986 test_loss : 3.1728246\n","train_loss : 3.1507757 test_loss : 3.1093585\n","train_loss : 3.154146 test_loss : 3.1814213\n","train_loss : 3.1543045 test_loss : 3.111614\n","train_loss : 3.1539645 test_loss : 3.1898727\n","train_loss : 3.1571271 test_loss : 3.1144423\n","train_loss : 3.1478002 test_loss : 3.1743639\n","train_loss : 3.151517 test_loss : 3.1111615\n","train_loss : 3.148717 test_loss : 3.1687183\n","train_loss : 3.1500766 test_loss : 3.1219406\n","train_loss : 3.1425865 test_loss : 3.1513116\n","train_loss : 3.1419084 test_loss : 3.1161368\n","train_loss : 3.1455777 test_loss : 3.154579\n","train_loss : 3.1441088 test_loss : 3.1233177\n","train_loss : 3.1436324 test_loss : 3.1738775\n","train_loss : 3.1520839 test_loss : 3.1161058\n","train_loss : 3.149191 test_loss : 3.1788194\n","train_loss : 3.1527452 test_loss : 3.1135159\n","train_loss : 3.152063 test_loss : 3.2017865\n","train_loss : 3.1628659 test_loss : 3.1169248\n","train_loss : 3.1515198 test_loss : 3.18625\n","train_loss : 3.1561167 test_loss : 3.1154099\n","train_loss : 3.1466115 test_loss : 3.1609037\n","train_loss : 3.146754 test_loss : 3.113255\n","train_loss : 3.1464145 test_loss : 3.164075\n","train_loss : 3.1481018 test_loss : 3.1097379\n","train_loss : 3.151445 test_loss : 3.1851482\n","train_loss : 3.1565442 test_loss : 3.1119933\n","train_loss : 3.1515527 test_loss : 3.1645863\n","train_loss : 3.148247 test_loss : 3.125336\n","train_loss : 3.1409001 test_loss : 3.1472087\n","train_loss : 3.1403952 test_loss : 3.121512\n","train_loss : 3.1417217 test_loss : 3.1595776\n","train_loss : 3.1445484 test_loss : 3.113915\n","train_loss : 3.1489499 test_loss : 3.1720967\n","train_loss : 3.1494482 test_loss : 3.1109023\n","train_loss : 3.1549716 test_loss : 3.1961787\n","train_loss : 3.1618686 test_loss : 3.1122754\n","train_loss : 3.1512346 test_loss : 3.1657043\n","train_loss : 3.1482716 test_loss : 3.1263921\n","train_loss : 3.1408072 test_loss : 3.1593003\n","train_loss : 3.144344 test_loss : 3.116337\n","train_loss : 3.14407 test_loss : 3.1648881\n","train_loss : 3.1477518 test_loss : 3.1107123\n","train_loss : 3.1521118 test_loss : 3.1932216\n","train_loss : 3.1597435 test_loss : 3.1141908\n","train_loss : 3.1491969 test_loss : 3.1720672\n","train_loss : 3.1500676 test_loss : 3.110136\n","train_loss : 3.1482084 test_loss : 3.1694822\n","train_loss : 3.1508403 test_loss : 3.1126032\n","train_loss : 3.1480882 test_loss : 3.1721666\n","train_loss : 3.1505678 test_loss : 3.1081405\n","train_loss : 3.1514542 test_loss : 3.1842155\n","train_loss : 3.1563396 test_loss : 3.1110218\n","train_loss : 3.1516206 test_loss : 3.170336\n","train_loss : 3.1503954 test_loss : 3.1084328\n","train_loss : 3.1491966 test_loss : 3.165541\n","train_loss : 3.149445 test_loss : 3.1228535\n","train_loss : 3.1429262 test_loss : 3.1518507\n","train_loss : 3.1418421 test_loss : 3.1171505\n","train_loss : 3.1460788 test_loss : 3.1607661\n","train_loss : 3.1460824 test_loss : 3.113029\n","train_loss : 3.1474829 test_loss : 3.1733701\n","train_loss : 3.1509433 test_loss : 3.1094766\n","train_loss : 3.1535115 test_loss : 3.184368\n","train_loss : 3.1567585 test_loss : 3.1110106\n","train_loss : 3.1513255 test_loss : 3.1644692\n","train_loss : 3.1481516 test_loss : 3.1248443\n","train_loss : 3.1409762 test_loss : 3.1469848\n","train_loss : 3.1405387 test_loss : 3.121063\n","train_loss : 3.1414723 test_loss : 3.1601958\n","train_loss : 3.1450095 test_loss : 3.1135707\n","train_loss : 3.1486669 test_loss : 3.172761\n","train_loss : 3.1498961 test_loss : 3.110228\n","train_loss : 3.1549377 test_loss : 3.1962416\n","train_loss : 3.1620252 test_loss : 3.1118803\n","train_loss : 3.1511035 test_loss : 3.1655824\n","train_loss : 3.1482394 test_loss : 3.1104867\n","train_loss : 3.1459637 test_loss : 3.1546674\n","train_loss : 3.1466627 test_loss : 3.1203756\n","train_loss : 3.14166 test_loss : 3.1467822\n","train_loss : 3.1405938 test_loss : 3.1407003\n","train_loss : 3.1390247 test_loss : 3.143074\n","train_loss : 3.1391819 test_loss : 3.1334693\n","train_loss : 3.1395364 test_loss : 3.1334887\n","train_loss : 3.1394172 test_loss : 3.1552672\n","train_loss : 3.1417992 test_loss : 3.116903\n","train_loss : 3.145406 test_loss : 3.1624153\n","train_loss : 3.1464372 test_loss : 3.1135585\n","train_loss : 3.1491473 test_loss : 3.176593\n","train_loss : 3.1526413 test_loss : 3.1109977\n","train_loss : 3.1566317 test_loss : 3.1948223\n","train_loss : 3.1606698 test_loss : 3.1133766\n","train_loss : 3.1534781 test_loss : 3.1792862\n","train_loss : 3.1540527 test_loss : 3.1118617\n","train_loss : 3.1481574 test_loss : 3.1634672\n","train_loss : 3.148482 test_loss : 3.1192977\n","train_loss : 3.143649 test_loss : 3.1618857\n","train_loss : 3.1474195 test_loss : 3.1127696\n","train_loss : 3.1446183 test_loss : 3.151014\n","train_loss : 3.142635 test_loss : 3.1224408\n","train_loss : 3.1415749 test_loss : 3.1435704\n","train_loss : 3.140834 test_loss : 3.1198416\n","train_loss : 3.1417322 test_loss : 3.1529055\n","train_loss : 3.141957 test_loss : 3.11263\n","train_loss : 3.1473832 test_loss : 3.1723943\n","train_loss : 3.1519415 test_loss : 3.107753\n","train_loss : 3.153064 test_loss : 3.187072\n","train_loss : 3.1582298 test_loss : 3.1103308\n","train_loss : 3.150081 test_loss : 3.1648614\n","train_loss : 3.1492784 test_loss : 3.11195\n","train_loss : 3.1446362 test_loss : 3.1431913\n","train_loss : 3.1431441 test_loss : 3.1174612\n","train_loss : 3.141455 test_loss : 3.1516676\n","train_loss : 3.1433275 test_loss : 3.1097946\n","train_loss : 3.1510103 test_loss : 3.1858163\n","train_loss : 3.156991 test_loss : 3.11164\n","train_loss : 3.1519148 test_loss : 3.2005572\n","train_loss : 3.1628745 test_loss : 3.1138244\n","train_loss : 3.150227 test_loss : 3.168916\n","train_loss : 3.149208 test_loss : 3.1131785\n","train_loss : 3.1453185 test_loss : 3.1517534\n","train_loss : 3.1458755 test_loss : 3.1159859\n","train_loss : 3.1419988 test_loss : 3.1501102\n","train_loss : 3.1422534 test_loss : 3.1110942\n","train_loss : 3.1452808 test_loss : 3.1598072\n","train_loss : 3.1492357 test_loss : 3.106738\n","train_loss : 3.1533012 test_loss : 3.1876776\n","train_loss : 3.1591322 test_loss : 3.1087613\n","train_loss : 3.1497443 test_loss : 3.171952\n","train_loss : 3.1522257 test_loss : 3.1065373\n","train_loss : 3.1522734 test_loss : 3.1870632\n","train_loss : 3.1582682 test_loss : 3.1092205\n","train_loss : 3.150167 test_loss : 3.164811\n","train_loss : 3.149416 test_loss : 3.1240714\n","train_loss : 3.1414218 test_loss : 3.1505787\n","train_loss : 3.1404226 test_loss : 3.1181636\n","train_loss : 3.1473238 test_loss : 3.1619072\n","train_loss : 3.1468801 test_loss : 3.1228843\n","train_loss : 3.1436408 test_loss : 3.168089\n","train_loss : 3.1493852 test_loss : 3.1141105\n","train_loss : 3.1501098 test_loss : 3.192661\n","train_loss : 3.1582837 test_loss : 3.1163754\n","train_loss : 3.15005 test_loss : 3.1921482\n","train_loss : 3.157918 test_loss : 3.1164818\n","train_loss : 3.1448493 test_loss : 3.1552942\n","train_loss : 3.1445765 test_loss : 3.1238089\n","train_loss : 3.141867 test_loss : 3.1602604\n","train_loss : 3.145028 test_loss : 3.1150699\n","train_loss : 3.1494896 test_loss : 3.196069\n","train_loss : 3.1594756 test_loss : 3.1178577\n","train_loss : 3.1496365 test_loss : 3.1588848\n","train_loss : 3.1450815 test_loss : 3.1137614\n","train_loss : 3.147001 test_loss : 3.1765728\n","train_loss : 3.1518028 test_loss : 3.117014\n","train_loss : 3.1507864 test_loss : 3.1788268\n","train_loss : 3.1509452 test_loss : 3.114651\n","train_loss : 3.1518955 test_loss : 3.1937401\n","train_loss : 3.1584866 test_loss : 3.1161847\n","train_loss : 3.1499424 test_loss : 3.1723652\n","train_loss : 3.149518 test_loss : 3.1161451\n","train_loss : 3.1437073 test_loss : 3.147181\n","train_loss : 3.1422112 test_loss : 3.1247325\n","train_loss : 3.1412518 test_loss : 3.14648\n","train_loss : 3.1407363 test_loss : 3.1389635\n","train_loss : 3.138998 test_loss : 3.1384943\n","train_loss : 3.139506 test_loss : 3.1374958\n","train_loss : 3.1398385 test_loss : 3.1360633\n","train_loss : 3.1391325 test_loss : 3.1384416\n","train_loss : 3.1387782 test_loss : 3.1394584\n","train_loss : 3.139135 test_loss : 3.1299582\n","train_loss : 3.1392736 test_loss : 3.1322672\n","train_loss : 3.1395948 test_loss : 3.1530828\n","train_loss : 3.141535 test_loss : 3.1172912\n","train_loss : 3.1418233 test_loss : 3.149933\n","train_loss : 3.1423516 test_loss : 3.1214092\n","train_loss : 3.1456892 test_loss : 3.1755762\n","train_loss : 3.1524487 test_loss : 3.1113577\n","train_loss : 3.153033 test_loss : 3.2011464\n","train_loss : 3.1649811 test_loss : 3.1116958\n","train_loss : 3.159853 test_loss : 3.208841\n","train_loss : 3.1684175 test_loss : 3.1119323\n","train_loss : 3.1666253 test_loss : 3.2197669\n","train_loss : 3.17558 test_loss : 3.111293\n","train_loss : 3.1690605 test_loss : 3.225405\n","train_loss : 3.180151 test_loss : 3.1093256\n","train_loss : 3.1677 test_loss : 3.1819046\n","train_loss : 3.1600223 test_loss : 3.103102\n","train_loss : 3.1475003 test_loss : 3.1426342\n","train_loss : 3.14567 test_loss : 3.1135843\n","train_loss : 3.1425893 test_loss : 3.1539292\n","train_loss : 3.1455805 test_loss : 3.1068342\n","train_loss : 3.146135 test_loss : 3.1551027\n","train_loss : 3.1476548 test_loss : 3.114746\n","train_loss : 3.1438956 test_loss : 3.1553206\n","train_loss : 3.146133 test_loss : 3.106441\n","train_loss : 3.1490812 test_loss : 3.164812\n","train_loss : 3.1494608 test_loss : 3.1085508\n","train_loss : 3.1510215 test_loss : 3.1774983\n","train_loss : 3.1547089 test_loss : 3.1078293\n","train_loss : 3.153848 test_loss : 3.193033\n","train_loss : 3.1605067 test_loss : 3.1121893\n","train_loss : 3.1455846 test_loss : 3.151395\n","train_loss : 3.144989 test_loss : 3.1194122\n","train_loss : 3.1425653 test_loss : 3.1610548\n","train_loss : 3.1463082 test_loss : 3.112463\n","train_loss : 3.1452868 test_loss : 3.1618605\n","train_loss : 3.1488771 test_loss : 3.1080701\n","train_loss : 3.152801 test_loss : 3.1829882\n","train_loss : 3.1557963 test_loss : 3.1103256\n","train_loss : 3.152645 test_loss : 3.1760736\n","train_loss : 3.152986 test_loss : 3.1095326\n","train_loss : 3.1492174 test_loss : 3.1672466\n","train_loss : 3.1497936 test_loss : 3.1165779\n","train_loss : 3.146028 test_loss : 3.1644478\n","train_loss : 3.1490498 test_loss : 3.1120164\n","train_loss : 3.146107 test_loss : 3.1743252\n","train_loss : 3.1529963 test_loss : 3.1098619\n","train_loss : 3.1556659 test_loss : 3.1927016\n","train_loss : 3.159309 test_loss : 3.1134088\n","train_loss : 3.149081 test_loss : 3.176976\n","train_loss : 3.1531396 test_loss : 3.111355\n","train_loss : 3.1486483 test_loss : 3.1680403\n","train_loss : 3.1501296 test_loss : 3.1105833\n","train_loss : 3.147312 test_loss : 3.1707032\n","train_loss : 3.1507025 test_loss : 3.113035\n","train_loss : 3.1463518 test_loss : 3.1448448\n","train_loss : 3.1432266 test_loss : 3.1204073\n","train_loss : 3.140862 test_loss : 3.146518\n","train_loss : 3.141083 test_loss : 3.1162698\n","train_loss : 3.143976 test_loss : 3.1533155\n","train_loss : 3.1434197 test_loss : 3.1099513\n","train_loss : 3.1574004 test_loss : 3.193125\n","train_loss : 3.1604068 test_loss : 3.1124988\n","train_loss : 3.1489153 test_loss : 3.1715848\n","train_loss : 3.1501114 test_loss : 3.108273\n","train_loss : 3.1503716 test_loss : 3.1644032\n","train_loss : 3.1484156 test_loss : 3.1117408\n","train_loss : 3.145663 test_loss : 3.144631\n","train_loss : 3.1433792 test_loss : 3.1153593\n","train_loss : 3.1421623 test_loss : 3.1488783\n","train_loss : 3.1416233 test_loss : 3.1106517\n","train_loss : 3.1456816 test_loss : 3.1657255\n","train_loss : 3.1489482 test_loss : 3.1101005\n","train_loss : 3.1504307 test_loss : 3.1721466\n","train_loss : 3.1521537 test_loss : 3.106783\n","train_loss : 3.1549537 test_loss : 3.1908529\n","train_loss : 3.1596985 test_loss : 3.1102788\n","train_loss : 3.149057 test_loss : 3.1687837\n","train_loss : 3.150261 test_loss : 3.1066482\n","train_loss : 3.1473296 test_loss : 3.1683009\n","train_loss : 3.1508605 test_loss : 3.1062999\n","train_loss : 3.1505764 test_loss : 3.1607623\n","train_loss : 3.1485293 test_loss : 3.1192408\n","train_loss : 3.1431594 test_loss : 3.1548276\n","train_loss : 3.1441214 test_loss : 3.1134083\n","train_loss : 3.1468232 test_loss : 3.1597857\n","train_loss : 3.1470942 test_loss : 3.1103468\n","train_loss : 3.1481242 test_loss : 3.1756613\n","train_loss : 3.154213 test_loss : 3.1069617\n","train_loss : 3.154648 test_loss : 3.1848438\n","train_loss : 3.157146 test_loss : 3.1104305\n","train_loss : 3.1509933 test_loss : 3.163848\n","train_loss : 3.148727 test_loss : 3.1237102\n","train_loss : 3.1407683 test_loss : 3.1463108\n","train_loss : 3.140603 test_loss : 3.118175\n","train_loss : 3.1448832 test_loss : 3.1593945\n","train_loss : 3.1443732 test_loss : 3.1142495\n","train_loss : 3.1516707 test_loss : 3.179774\n","train_loss : 3.1533887 test_loss : 3.1119015\n","train_loss : 3.155554 test_loss : 3.1971607\n","train_loss : 3.161398 test_loss : 3.1137688\n","train_loss : 3.1514146 test_loss : 3.1667817\n","train_loss : 3.1485062 test_loss : 3.1268125\n","train_loss : 3.14074 test_loss : 3.141841\n","train_loss : 3.1396942 test_loss : 3.1406498\n","train_loss : 3.1392193 test_loss : 3.1252992\n","train_loss : 3.1408873 test_loss : 3.1453187\n","train_loss : 3.141757 test_loss : 3.122821\n","train_loss : 3.141765 test_loss : 3.1594155\n","train_loss : 3.1447973 test_loss : 3.11604\n","train_loss : 3.150256 test_loss : 3.1733708\n","train_loss : 3.1527982 test_loss : 3.1137922\n","train_loss : 3.148329 test_loss : 3.1723413\n","train_loss : 3.1506293 test_loss : 3.1095665\n","train_loss : 3.1499403 test_loss : 3.1808896\n","train_loss : 3.1546047 test_loss : 3.1110868\n","train_loss : 3.153825 test_loss : 3.1964734\n","train_loss : 3.1604066 test_loss : 3.1141596\n","train_loss : 3.1496048 test_loss : 3.1707103\n","train_loss : 3.1498697 test_loss : 3.1105287\n","train_loss : 3.1478136 test_loss : 3.1691499\n","train_loss : 3.1509433 test_loss : 3.109757\n","train_loss : 3.1464303 test_loss : 3.1672995\n","train_loss : 3.1495776 test_loss : 3.1118364\n","train_loss : 3.1471715 test_loss : 3.156669\n","train_loss : 3.1474469 test_loss : 3.1209228\n","train_loss : 3.1411047 test_loss : 3.142537\n","train_loss : 3.1400082 test_loss : 3.1379526\n","train_loss : 3.13905 test_loss : 3.1533773\n","train_loss : 3.1412296 test_loss : 3.1293178\n","train_loss : 3.1399982 test_loss : 3.150434\n","train_loss : 3.1407669 test_loss : 3.1170888\n","train_loss : 3.141877 test_loss : 3.1505225\n","train_loss : 3.142747 test_loss : 3.1207078\n","train_loss : 3.1478434 test_loss : 3.1771917\n","train_loss : 3.1525009 test_loss : 3.1129084\n","train_loss : 3.1555517 test_loss : 3.1990604\n","train_loss : 3.1624458 test_loss : 3.1148677\n","train_loss : 3.1522138 test_loss : 3.1995087\n","train_loss : 3.1613126 test_loss : 3.1176615\n","train_loss : 3.147736 test_loss : 3.1618755\n","train_loss : 3.146948 test_loss : 3.124642\n","train_loss : 3.1416821 test_loss : 3.155471\n","train_loss : 3.1432815 test_loss : 3.1213593\n","train_loss : 3.142368 test_loss : 3.15776\n","train_loss : 3.1434348 test_loss : 3.1152303\n","train_loss : 3.1443036 test_loss : 3.1580126\n","train_loss : 3.1448255 test_loss : 3.1228766\n","train_loss : 3.1449368 test_loss : 3.1737018\n","train_loss : 3.1500216 test_loss : 3.117165\n","train_loss : 3.152001 test_loss : 3.2083619\n","train_loss : 3.1634429 test_loss : 3.1168454\n","train_loss : 3.1551344 test_loss : 3.198709\n","train_loss : 3.1590908 test_loss : 3.119434\n","train_loss : 3.1465049 test_loss : 3.1591215\n","train_loss : 3.14616 test_loss : 3.1222436\n","train_loss : 3.142847 test_loss : 3.1626916\n","train_loss : 3.1463127 test_loss : 3.1155589\n","train_loss : 3.1455305 test_loss : 3.1634727\n","train_loss : 3.1485977 test_loss : 3.1104436\n","train_loss : 3.1512556 test_loss : 3.1885679\n","train_loss : 3.1563065 test_loss : 3.1133847\n","train_loss : 3.1522427 test_loss : 3.1889386\n","train_loss : 3.155951 test_loss : 3.1161926\n","train_loss : 3.146679 test_loss : 3.1629283\n","train_loss : 3.1478617 test_loss : 3.1222804\n","train_loss : 3.1426656 test_loss : 3.1581838\n","train_loss : 3.1437955 test_loss : 3.1154742\n","train_loss : 3.14354 test_loss : 3.1520803\n","train_loss : 3.1438072 test_loss : 3.1184149\n","train_loss : 3.1468105 test_loss : 3.1717682\n","train_loss : 3.151138 test_loss : 3.1145172\n","train_loss : 3.1527889 test_loss : 3.2042584\n","train_loss : 3.1626387 test_loss : 3.1172833\n","train_loss : 3.1506312 test_loss : 3.1955874\n","train_loss : 3.1576853 test_loss : 3.1195035\n","train_loss : 3.145543 test_loss : 3.1646962\n","train_loss : 3.1459637 test_loss : 3.1143897\n","train_loss : 3.146148 test_loss : 3.1744235\n","train_loss : 3.1507504 test_loss : 3.111315\n","train_loss : 3.1569982 test_loss : 3.1962397\n","train_loss : 3.160467 test_loss : 3.1136405\n","train_loss : 3.1538458 test_loss : 3.1967998\n","train_loss : 3.1596882 test_loss : 3.1168973\n","train_loss : 3.1459355 test_loss : 3.154489\n","train_loss : 3.145139 test_loss : 3.1226325\n","train_loss : 3.1429844 test_loss : 3.1639674\n","train_loss : 3.1456723 test_loss : 3.1158652\n","train_loss : 3.145852 test_loss : 3.1815941\n","train_loss : 3.1538389 test_loss : 3.111859\n","train_loss : 3.1554081 test_loss : 3.1963356\n","train_loss : 3.1596406 test_loss : 3.115562\n","train_loss : 3.1489687 test_loss : 3.1740832\n","train_loss : 3.1501083 test_loss : 3.114716\n","train_loss : 3.144038 test_loss : 3.15736\n","train_loss : 3.1465762 test_loss : 3.12187\n","train_loss : 3.1412108 test_loss : 3.1382384\n"]}]},{"cell_type":"markdown","metadata":{"id":"2u32IQUdYX-4"},"source":["Visualisation de l'évolution du coût pendant l'entraînement."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":449},"id":"03l20groTpLh","outputId":"59cb5b15-5db8-4d31-92ed-2824caa1c753","executionInfo":{"status":"ok","timestamp":1730127600850,"user_tz":-60,"elapsed":957,"user":{"displayName":"Gautherot Morgan","userId":"07974205866322024288"}}},"source":["plt.plot(np.arange(epoch), history_train, label='train loss')\n","plt.plot(np.arange(epoch), history_test, label='test loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('MAE')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"3pKkixYFY1qd"},"source":["## Sauvegarde du modèle"]},{"cell_type":"markdown","metadata":{"id":"mM_7mFShZ906"},"source":["Maintenant que le modèle est entraîné, il est temps de le sauvegarder."]},{"cell_type":"markdown","source":["Sauvegardez les poids du modèle grâce à la méthode `save`.\n","\n","N'hésitez pas à vous aider de la [documentation](https://pytorch.org/tutorials/beginner/saving_loading_models.html)"],"metadata":{"id":"YUoF8P2aiSsC"}},{"cell_type":"code","source":["torch.save(rl_model.state_dict(), 'linear_regression.pth')"],"metadata":{"id":"UrqYDKQvlqKI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jseO56VGaDeV"},"source":["Remplacez le modèle entraîné par un nouveau modèle dont les poids sont initialisés aléatoirement."]},{"cell_type":"code","metadata":{"id":"4zgfnYkWZh4z"},"source":["rl_model = linear_regression(input_shape=13, output_shape=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Calculez la performance de ce modèle."],"metadata":{"id":"aneouhUZobor"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0I2c9GpfZpWc","outputId":"2ba40b03-12ea-4499-9725-ef9d6ec9287c","executionInfo":{"status":"ok","timestamp":1730127600850,"user_tz":-60,"elapsed":4,"user":{"displayName":"Gautherot Morgan","userId":"07974205866322024288"}}},"source":["prediction = rl_model(x_train_torch)\n","criterion(prediction, y_train_torch)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(23.1336, grad_fn=<MeanBackward0>)"]},"metadata":{},"execution_count":34}]},{"cell_type":"markdown","source":["Comme les paramètres du modèle sont initialisés aléatoirement, les performances sont faibles."],"metadata":{"id":"Np-F09gxh_Kl"}},{"cell_type":"markdown","metadata":{"id":"C92sBoBvaJuW"},"source":["Nous pouvons restaurer nos poids entraînés en chargeant les poids précédemment sauvegardés.\n","\n","En utilisant la méthode `load_state_dict`.\n","\n","Aidez-vous de la [documentation](https://pytorch.org/tutorials/beginner/saving_loading_models.html)"]},{"cell_type":"code","metadata":{"id":"64FFwodfZZGc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730127600851,"user_tz":-60,"elapsed":5,"user":{"displayName":"Gautherot Morgan","userId":"07974205866322024288"}},"outputId":"873cfe68-6ff0-4d89-9eb1-b77a92f94bb8"},"source":["rl_model.load_state_dict(torch.load('linear_regression.pth', weights_only=True))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T7E-voudZqoy","outputId":"c77c61d4-0407-4f56-cefa-d1f823c74120","executionInfo":{"status":"ok","timestamp":1730127600851,"user_tz":-60,"elapsed":4,"user":{"displayName":"Gautherot Morgan","userId":"07974205866322024288"}}},"source":["prediction = rl_model.forward(x_train_torch)\n","criterion(prediction, y_train_torch)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(3.1398, grad_fn=<MeanBackward0>)"]},"metadata":{},"execution_count":36}]},{"cell_type":"markdown","source":["Maintenant que les paramètres du modèle entraîné sont chargés, ses performances sont identiques au précédent modèle."],"metadata":{"id":"AfCACAbDh8aV"}},{"cell_type":"code","source":[],"metadata":{"id":"k5GScsdMo69t"},"execution_count":null,"outputs":[]}]}